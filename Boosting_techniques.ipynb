{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4tMSR6nlAwp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thoery Answers:\n",
        "\n",
        "\n",
        "1. What is Boosting in Machine Learning?\n",
        "\n",
        " Boosting is an ensemble learning technique that combines multiple weak learners (usually decision trees) to form a strong learner. It works by training models sequentially, where each model tries to correct the errors made by the previous one. This iterative process reduces bias and variance, improving overall model accuracy.\n",
        "\n",
        "\n",
        "2. How does Boosting differ from Bagging?\n",
        "\n",
        "Boosting trains models sequentially, where each model focuses on the mistakes of the previous one. It reduces bias and improves performance.\n",
        "Bagging trains models in parallel on different subsets of data to reduce variance and prevent overfitting (e.g., Random Forest).\n",
        "\n",
        "\n",
        "3. What is the key idea behind AdaBoost?\n",
        "\n",
        "AdaBoost (Adaptive Boosting) assigns higher weights to misclassified samples so that subsequent weak learners focus on them. The final model is a weighted sum of all weak learners.\n",
        "\n",
        "\n",
        "4. Explain the working of AdaBoost with an example.\n",
        "\n",
        "Step 1: Assign equal weights to all training samples.\n",
        "\n",
        "Step 2: Train a weak learner (e.g., a decision stump).\n",
        "\n",
        "Step 3: Compute errors and increase weights of misclassified samples.\n",
        "\n",
        "Step 4: Train the next weak learner on the updated dataset.\n",
        "\n",
        "Step 5: Repeat until the desired number of weak learners is trained.\n",
        "\n",
        "Step 6: Make predictions using a weighted sum of weak learners.\n",
        "\n",
        "\n",
        "5. What is Gradient Boosting, and how is it different from AdaBoost?\n",
        "\n",
        "Gradient Boosting builds models to correct the residual errors of previous models using gradient descent. Unlike AdaBoost, it does not adjust sample weights but instead minimizes a differentiable loss function.\n",
        "\n",
        "\n",
        "6. What is the loss function in Gradient Boosting?\n",
        "\n",
        "Regression: Mean Squared Error (MSE), Huber loss\n",
        "\n",
        "Classification: Log Loss (Cross-Entropy)\n",
        "\n",
        "\n",
        "7. How does XGBoost improve over traditional Gradient Boosting?\n",
        "Regularization (L1 & L2) to reduce overfitting.\n",
        "\n",
        "Tree pruning (max depth) for better generalization.\n",
        "\n",
        "Handling missing values automatically.\n",
        "\n",
        "Parallelization for faster training.\n",
        "\n",
        "Weighted quantile sketch for better split finding.\n",
        "\n",
        "\n",
        "8. What is the difference between XGBoost and CatBoost?\n",
        "\n",
        "XGBoost is optimized for numerical and categorical data but requires manual encoding.\n",
        "CatBoost is optimized for categorical data and uses efficient encoding with ordered boosting, making it faster and more accurate for such datasets.\n",
        "\n",
        "\n",
        "9. What are some real-world applications of Boosting techniques?\n",
        "\n",
        "Finance: Fraud detection, credit scoring\n",
        "\n",
        "Healthcare: Disease prediction\n",
        "\n",
        "Marketing: Customer churn prediction\n",
        "\n",
        "E-commerce: Product recommendations\n",
        "\n",
        "Cybersecurity: Anomaly detection in network traffic\n",
        "\n",
        "\n",
        "10. How does regularization help in XGBoost?\n",
        "\n",
        "L1 (Lasso): Shrinks less important features to zero.\n",
        "\n",
        "L2 (Ridge): Prevents large weights, reducing model complexity.\n",
        "\n",
        "Helps prevent overfitting and improves generalization.\n",
        "\n",
        "\n",
        "\n",
        "11. What are some hyperparameters to tune in Gradient Boosting models?\n",
        "\n",
        "Learning rate (eta): Step size (0.01–0.3).\n",
        "\n",
        "Number of estimators: Number of trees (100–1000).\n",
        "\n",
        "Max depth: Controls tree depth (3–10).\n",
        "\n",
        "Subsample: Fraction of data per tree (0.5–1.0).\n",
        "\n",
        "Colsample_bytree: Features per tree (0.3–1.0).\n",
        "\n",
        "Min_child_weight: Minimum sum of instance weights for a split.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "12. What is the concept of Feature Importance in Boosting?\n",
        "\n",
        "Feature importance measures how much each feature contributes to predictions. Methods include:\n",
        "\n",
        "\n",
        "Gain: Contribution to loss reduction.\n",
        "Frequency: Number of times a feature is used for splits.\n",
        "SHAP values: Advanced method showing impact on predictions.\n",
        "Why is CatBoost efficient for categorical data?\n",
        "\n",
        "\n",
        "13. Why is catboost efficient for categorical data?\n",
        "Ordered boosting: Reduces overfitting caused by target leakage.\n",
        "Efficient encoding: Uses a unique algorithm instead of one-hot encoding.\n",
        "Oblivious trees: Maintain symmetry for fast and stable training.\n"
      ],
      "metadata": {
        "id": "xvsdLJ68lBjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical questions"
      ],
      "metadata": {
        "id": "rovS5Dqam9JG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import required libraries\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, mean_absolute_error, mean_squared_error,\n",
        "    r2_score, classification_report, confusion_matrix,\n",
        "    roc_curve, auc, log_loss\n",
        ")\n",
        "\n",
        "from sklearn.ensemble import (\n",
        "    AdaBoostClassifier, AdaBoostRegressor,\n",
        "    GradientBoostingClassifier, GradientBoostingRegressor\n",
        ")\n",
        "\n",
        "from xgboost import XGBClassifier, XGBRegressor, plot_importance\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer, make_classification, make_regression\n",
        "\n",
        "# Generate datasets\n",
        "X_cls, y_cls = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_reg, y_reg = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "breast_cancer = load_breast_cancer()\n",
        "\n",
        "# Split datasets\n",
        "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_cls, y_cls, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg,\n",
        "y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_bc, X_test_bc, y_train_bc, y_test_bc = train_test_split(breast_cancer.data, breast_cancer.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# 14. Train an AdaBoost Classifier and print accuracy\n",
        "ada_cls = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "ada_cls.fit(X_train_cls, y_train_cls)\n",
        "\n",
        "y_pred_ada_cls = ada_cls.predict(X_test_cls)\n",
        "\n",
        "print(f\"14. AdaBoost Classifier Accuracy: {accuracy_score(y_test_cls, y_pred_ada_cls):.4f}\")\n",
        "\n",
        "# 15. Train an AdaBoost Regressor and evaluate using MAE\n",
        "ada_reg = AdaBoostRegressor(n_estimators=50, random_state=42)\n",
        "\n",
        "ada_reg.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "y_pred_ada_reg = ada_reg.predict(X_test_reg)\n",
        "\n",
        "print(f\"15. AdaBoost Regressor MAE: {mean_absolute_error(y_test_reg, y_pred_ada_reg):.4f}\")\n",
        "\n",
        "# 16. Train Gradient Boosting Classifier on Breast Cancer dataset and print feature importance\n",
        "gbc = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "gbc.fit(X_train_bc, y_train_bc)\n",
        "\n",
        "print(\"16. Feature Importance (Gradient Boosting Classifier):\", gbc.feature_importances_)\n",
        "\n",
        "# 17. Train a Gradient Boosting Regressor and evaluate using R-Squared Score\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "gbr.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "y_pred_gbr = gbr.predict(X_test_reg)\n",
        "\n",
        "print(f\"17. Gradient Boosting Regressor R-Squared Score: {r2_score(y_test_reg, y_pred_gbr):.4f}\")\n",
        "\n",
        "# 18. Train an XGBoost Classifier and compare accuracy with Gradient Boosting\n",
        "xgb_cls = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "xgb_cls.fit(X_train_cls, y_train_cls)\n",
        "\n",
        "y_pred_xgb_cls = xgb_cls.predict(X_test_cls)\n",
        "\n",
        "print(f\"18. XGBoost Classifier Accuracy: {accuracy_score(y_test_cls, y_pred_xgb_cls):.4f}\")\n",
        "\n",
        "# 19. Train a CatBoost Classifier and evaluate using F1-Score\n",
        "cat_cls = CatBoostClassifier(iterations=100, verbose=0, random_state=42)\n",
        "\n",
        "cat_cls.fit(X_train_cls, y_train_cls)\n",
        "\n",
        "y_pred_cat_cls = cat_cls.predict(X_test_cls)\n",
        "\n",
        "print(f\"19. CatBoost Classifier F1-Score: {classification_report(y_test_cls, y_pred_cat_cls)}\")\n",
        "\n",
        "# 20. Train an XGBoost Regressor and evaluate using MSE\n",
        "xgb_reg = XGBRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "xgb_reg.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "y_pred_xgb_reg = xgb_reg.predict(X_test_reg)\n",
        "\n",
        "print(f\"20. XGBoost Regressor MSE: {mean_squared_error(y_test_reg, y_pred_xgb_reg):.4f}\")\n",
        "\n",
        "# 21. Train AdaBoost Classifier and visualize feature importance\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "plt.bar(range(len(ada_cls.feature_importances_)), ada_cls.feature_importances_)\n",
        "\n",
        "plt.title(\"21. AdaBoost Feature Importance\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# 22. Train Gradient Boosting Regressor and plot learning curves\n",
        "train_sizes = np.linspace(0.1, 1.0, 10)\n",
        "\n",
        "train_scores, test_scores = [], []\n",
        "\n",
        "for size in train_sizes:\n",
        "    X_train_sample, _, y_train_sample, _ = train_test_split(X_train_reg,\n",
        "    y_train_reg, train_size=size, random_state=42)\n",
        "    \n",
        "    gbr.fit(X_train_sample, y_train_sample)\n",
        "    \n",
        "    train_scores.append(gbr.score(X_train_sample, y_train_sample))\n",
        "    \n",
        "    test_scores.append(gbr.score(X_test_reg, y_test_reg))\n",
        "\n",
        "plt.plot(train_sizes, train_scores, label=\"Training Score\")\n",
        "\n",
        "plt.plot(train_sizes, test_scores, label=\"Validation Score\")\n",
        "\n",
        "plt.title(\"22. Learning Curve - Gradient Boosting Regressor\")\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# 23. Train XGBoost Classifier and visualize feature importance\n",
        "plot_importance(xgb_cls, max_num_features=10, title=\"23. XGBoost Feature Importance\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# 24. Train CatBoost Classifier and plot confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test_cls, y_pred_cat_cls)\n",
        "\n",
        "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "\n",
        "plt.title(\"24. CatBoost Confusion Matrix\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# 25. Train AdaBoost Classifier with different estimators and compare accuracy\n",
        "estimators = [10, 50, 100, 200]\n",
        "\n",
        "accuracy_scores = []\n",
        "\n",
        "for n in estimators:\n",
        "    model = AdaBoostClassifier(n_estimators=n, random_state=42)\n",
        "    model.fit(X_train_cls, y_train_cls)\n",
        "    y_pred = model.predict(X_test_cls)\n",
        "    accuracy_scores.append(accuracy_score(y_test_cls, y_pred))\n",
        "\n",
        "plt.plot(estimators, accuracy_scores, marker='o')\n",
        "\n",
        "plt.title(\"25. AdaBoost Accuracy vs. Number of Estimators\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# 26. Train Gradient Boosting Classifier and visualize ROC curve\n",
        "y_probs = gbc.predict_proba(X_test_bc)[:, 1]\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test_bc, y_probs)\n",
        "\n",
        "plt.plot(fpr, tpr, label=f\"AUC: {auc(fpr, tpr):.2f}\")\n",
        "\n",
        "plt.title(\"26. Gradient Boosting ROC Curve\")\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# 27. Train XGBoost Regressor and tune learning rate using GridSearchCV\n",
        "params = {'learning_rate': [0.01, 0.1, 0.2, 0.3]}\n",
        "\n",
        "grid = GridSearchCV(XGBRegressor(n_estimators=100, random_state=42),\n",
        "param_grid=params, scoring='neg_mean_squared_error')\n",
        "\n",
        "grid.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "print(f\"27. Best Learning Rate for XGBoost: {grid.best_params_}\")\n",
        "\n",
        "# 28. Train CatBoost Classifier on imbalanced dataset and compare with class weighting\n",
        "cat_cls_weighted = CatBoostClassifier(iterations=100, class_weights=[1, 3], verbose=0, random_state=42)\n",
        "\n",
        "cat_cls_weighted.fit(X_train_cls, y_train_cls)\n",
        "\n",
        "y_pred_cat_weighted = cat_cls_weighted.predict(X_test_cls)\n",
        "\n",
        "print(f\"28. CatBoost Weighted Classification Report:\\n{classification_report(y_test_cls, y_pred_cat_weighted)}\")\n",
        "\n",
        "# 29. Train AdaBoost Classifier with different learning rates\n",
        "for lr in [0.01, 0.1, 0.5, 1.0]:\n",
        "    model = AdaBoostClassifier(learning_rate=lr, random_state=42)\n",
        "    model.fit(X_train_cls, y_train_cls)\n",
        "    print(f\"29. Learning Rate {lr}: Accuracy {accuracy_score(y_test_cls, model.predict(X_test_cls)):.4f}\")\n",
        "\n",
        "# 30. Train XGBoost Classifier for multi-class classification and evaluate log-loss\n",
        "print(f\"30. Log Loss: {log_loss(y_test_cls, xgb_cls.predict_proba(X_test_cls)):.4f}\")\n"
      ],
      "metadata": {
        "id": "ZRNVUnvim_yo"
      }
    }
  ]
}