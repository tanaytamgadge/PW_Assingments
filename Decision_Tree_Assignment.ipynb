{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MRTuGACTdiQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DECISION TREE ASSIGNMENT"
      ],
      "metadata": {
        "id": "NxBBt3eJTet-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Tree Theorotical Answers**\n",
        "\n",
        "1. **What is a Decision Tree, and how does it work?**  \n",
        "A Decision Tree is a supervised learning algorithm used for classification and regression. It consists of decision nodes, branches, and leaf nodes. The tree recursively splits data based on certain criteria (e.g., Gini Impurity or Entropy) to separate the data effectively.\n",
        "\n",
        "2. **What are impurity measures in Decision Trees?**  \n",
        "Impurity measures help evaluate the quality of a split in a Decision Tree. Common impurity measures include:\n",
        "   - Gini Impurity\n",
        "   - Entropy (Information Gain)\n",
        "\n",
        "3. **What is the mathematical formula for Gini Impurity?**  \n",
        "Gini = 1 - Σ (p_i)^2  \n",
        "where p_i is the probability of class i in the node and C is the total number of classes.\n",
        "\n",
        "4. **What is the mathematical formula for Entropy?**  \n",
        "Entropy = - Σ p_i log_2 (p_i)  \n",
        "where p_i is the probability of class i in the node.\n",
        "\n",
        "5. **What is Information Gain, and how is it used in Decision Trees?**  \n",
        "Information Gain (IG) measures the reduction in entropy after splitting a dataset. It helps determine the best feature for splitting.\n",
        "IG = Entropy(parent) - Σ (N_j/N) Entropy(child_j)  \n",
        "where N is the total number of samples and N_j is the number of samples in child node j.\n",
        "\n",
        "6. **What is the difference between Gini Impurity and Entropy?**  \n",
        "- **Gini Impurity** is faster to compute and measures misclassification.\n",
        "- **Entropy** involves logarithm calculations and measures disorder in data.\n",
        "- **Gini Impurity** is used in CART, while **Entropy** is used in ID3 and C4.5.\n",
        "\n",
        "7. **What is the mathematical explanation behind Decision Trees?**  \n",
        "A Decision Tree splits data using **Information Gain** (Entropy) or minimizes **Gini Impurity**. The process follows:\n",
        "1. Select the best feature using impurity measures.\n",
        "2. Recursively split data.\n",
        "3. Prune the tree to prevent overfitting.\n",
        "\n",
        "8. **What is Pre-Pruning in Decision Trees?**  \n",
        "Pre-Pruning stops the tree from growing too complex by setting conditions like:\n",
        "- Maximum depth\n",
        "- Minimum samples per leaf\n",
        "- Minimum Information Gain threshold\n",
        "\n",
        "9. **What is Post-Pruning in Decision Trees?**  \n",
        "Post-Pruning removes unnecessary branches from a fully grown tree using:\n",
        "- Cost Complexity Pruning (CCP)\n",
        "- Reduced Error Pruning\n",
        "\n",
        "10. **What is the difference between Pre-Pruning and Post-Pruning?**  \n",
        "- **Pre-Pruning** stops tree growth early to prevent overfitting but may cause underfitting.\n",
        "- **Post-Pruning** grows a full tree first and then trims unnecessary branches to reduce overfitting.\n",
        "\n",
        "11. **What is a Decision Tree Regressor?**  \n",
        "A Decision Tree Regressor is used for predicting continuous values. It splits data based on a feature threshold and predicts using the mean value of each leaf node.\n",
        "\n",
        "12. **What are the advantages and disadvantages of Decision Trees?**  \n",
        "**Advantages:**\n",
        "- Easy to interpret\n",
        "- Handles both numerical and categorical data\n",
        "- Minimal data preprocessing required\n",
        "- Can model non-linear relationships\n",
        "\n",
        "**Disadvantages:**\n",
        "- Prone to overfitting\n",
        "- Sensitive to small data changes\n",
        "- Computationally expensive for large datasets\n",
        "\n",
        "13. **How does a Decision Tree handle missing values?**  \n",
        "- Ignores missing values during training\n",
        "- Uses imputation (mean, median, mode)\n",
        "- Uses surrogate splits\n",
        "- Assigns missing values as a separate category\n",
        "\n",
        "14. **How does a Decision Tree handle categorical features?**  \n",
        "- Label Encoding (assigning numbers to categories)\n",
        "- One-Hot Encoding (binary columns for each category)\n",
        "- Direct splitting (used in some implementations)\n",
        "\n",
        "15. **What are some real-world applications of Decision Trees?**  \n",
        "- Medical Diagnosis\n",
        "- Customer Churn Prediction\n",
        "- Credit Risk Assessment\n",
        "- Fraud Detection\n",
        "- Spam Detection\n",
        "\n"
      ],
      "metadata": {
        "id": "E37IyGMNTjnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRACTICAL QUESTIONS"
      ],
      "metadata": {
        "id": "--5Nl6WOTxzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_iris, fetch_california_housing\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor,\n",
        "export_text, plot_tree\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import graphviz\n",
        "\n",
        "\n",
        "# Question 16: Train a Decision Tree Classifier on the Iris dataset and print model accuracy\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Question 17: Train a Decision Tree Classifier using Gini Impurity and print feature importances\n",
        "clf_gini = DecisionTreeClassifier(criterion='gini')\n",
        "\n",
        "clf_gini.fit(X_train, y_train)\n",
        "\n",
        "print(\"Feature Importances:\", clf_gini.feature_importances_)\n",
        "\n",
        "# Question 18: Train a Decision Tree Classifier using Entropy and print accuracy\n",
        "clf_entropy = DecisionTreeClassifier(criterion='entropy')\n",
        "\n",
        "clf_entropy.fit(X_train, y_train)\n",
        "\n",
        "y_pred_entropy = clf_entropy.predict(X_test)\n",
        "\n",
        "print(\"Accuracy with Entropy:\", accuracy_score(y_test, y_pred_entropy))\n",
        "\n",
        "# Question 19: Train a Decision Tree Regressor on a housing dataset and evaluate using MSE\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "X_housing, y_housing = housing.data, housing.target\n",
        "\n",
        "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(X_housing,\n",
        "y_housing, test_size=0.2, random_state=42)\n",
        "\n",
        "regressor = DecisionTreeRegressor()\n",
        "\n",
        "regressor.fit(X_train_h, y_train_h)\n",
        "\n",
        "y_pred_h = regressor.predict(X_test_h)\n",
        "\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test_h, y_pred_h))\n",
        "\n",
        "# Question 20: Visualize the Decision Tree\n",
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Question 21: Compare max depth of 3 vs fully grown tree\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3)\n",
        "\n",
        "clf_depth3.fit(X_train, y_train)\n",
        "\n",
        "y_pred_depth3 = clf_depth3.predict(X_test)\n",
        "\n",
        "print(\"Accuracy with max_depth=3:\", accuracy_score(y_test, y_pred_depth3))\n",
        "\n",
        "print(\"Accuracy with full tree:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Question 22: Train with min_samples_split=5\n",
        "clf_split5 = DecisionTreeClassifier(min_samples_split=5)\n",
        "\n",
        "clf_split5.fit(X_train, y_train)\n",
        "\n",
        "y_pred_split5 = clf_split5.predict(X_test)\n",
        "\n",
        "print(\"Accuracy with min_samples_split=5:\", accuracy_score(y_test, y_pred_split5))\n",
        "\n",
        "# Question 23: Apply feature scaling before training\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf_scaled = DecisionTreeClassifier()\n",
        "\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "\n",
        "print(\"Accuracy with scaling:\", accuracy_score(y_test, y_pred_scaled))\n",
        "\n",
        "# Question 24: Train a Decision Tree Classifier using One-vs-Rest (OVR) strategy\n",
        "clf_ovr = DecisionTreeClassifier()\n",
        "\n",
        "clf_ovr.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy with OVR:\", accuracy_score(y_test, clf_ovr.predict(X_test)))\n",
        "\n",
        "# Question 25: Display feature importance scores\n",
        "print(\"Feature Importance Scores:\", clf.feature_importances_)\n",
        "\n",
        "# Question 26: Compare Decision Tree Regressor with max_depth=5\n",
        "regressor_depth5 = DecisionTreeRegressor(max_depth=5)\n",
        "\n",
        "regressor_depth5.fit(X_train_h, y_train_h)\n",
        "\n",
        "y_pred_depth5 = regressor_depth5.predict(X_test_h)\n",
        "\n",
        "print(\"MSE with max_depth=5:\", mean_squared_error(y_test_h, y_pred_depth5))\n",
        "\n",
        "# Question 27: Apply Cost Complexity Pruning (CCP) and visualize effect\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "\n",
        "ccp_alphas = path.ccp_alphas\n",
        "\n",
        "pruned_clfs = [DecisionTreeClassifier(ccp_alpha=alpha).fit(X_train, y_train) for alpha in ccp_alphas]\n",
        "\n",
        "pruned_accuracies = [accuracy_score(y_test, clf.predict(X_test)) for clf in pruned_clfs]\n",
        "\n",
        "plt.plot(ccp_alphas, pruned_accuracies, marker='o', linestyle='--')\n",
        "\n",
        "plt.xlabel('CCP Alpha')\n",
        "\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.title('Effect of CCP on Accuracy')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Question 28: Evaluate using Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n",
        "\n",
        "# Question 29: Visualize confusion matrix\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "\n",
        "plt.xlabel('Predicted')\n",
        "\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Question 30: Use GridSearchCV to find optimal max_depth and min_samples_split\n",
        "param_grid = {'max_depth': [3, 5, 10, None], 'min_samples_split': [2, 5, 10]}\n",
        "\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n"
      ],
      "metadata": {
        "id": "ZwSQMg-FTrya"
      }
    }
  ]
}