{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7pfokV61BWB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theory Questions:\n",
        "\n",
        "1. What is K-Nearest Neighbors (KNN) and how does it work?\n",
        "\n",
        "KNN is a supervised machine learning algorithm used for classification and regression. It works by storing the training data and predicting the output based on the majority label (for classification) or average (for regression) of the K closest data points (neighbors) in the feature space, using a distance metric like Euclidean distance.\n",
        "\n",
        "\n",
        "2. What is the difference between KNN Classification and KNN Regression?\n",
        "\n",
        "KNN Classification predicts the class label based on majority voting among the K nearest neighbors.\n",
        "KNN Regression predicts a continuous value by averaging the outputs of the K nearest neighbors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3. What is the role of the distance metric in KNN?\n",
        "\n",
        "The distance metric determines how \"close\" data points are. Common metrics:\n",
        "Euclidean Distance (default for continuous data)\n",
        "Manhattan Distance\n",
        "Minkowski Distance\n",
        "\n",
        "The accuracy of KNN heavily depends on the choice of an appropriate distance metric.\n",
        "\n",
        "4. What is the Curse of Dimensionality in KNN?\n",
        "\n",
        "As the number of dimensions (features) increases, the data becomes sparse, and all points start appearing equidistant. This degrades the performance of KNN, as distance metrics lose their effectiveness.\n",
        "\n",
        "5. How can we choose the best value of K in KNN?\n",
        "\n",
        "Use cross-validation to test various values of K.\n",
        "A small K can be noisy and overfit.\n",
        "A large K can smooth out predictions but may underfit.\n",
        "Typically, odd values of K are preferred to avoid ties in classification.\n",
        "\n",
        "6. What are KD Tree and Ball Tree in KNN?\n",
        "\n",
        "KD Tree: A space-partitioning tree data structure that organizes points in K-dimensional space using axis-aligned splits.\n",
        "Ball Tree: A binary tree where each node represents a n-dimensional ball (region), useful for non-axis aligned and high-dimensional data.\n",
        "\n",
        "7. When should you use KD Tree vs. Ball Tree?\n",
        "\n",
        "Use KD Tree for low-dimensional data (usually < 20 dimensions).\n",
        "Use Ball Tree for high-dimensional data or data where KD Tree performance degrades.\n",
        "\n",
        "8. What are the disadvantages of KNN?\n",
        "\n",
        "Computationally expensive at prediction time.\n",
        "\n",
        "Sensitive to irrelevant features and feature scaling.\n",
        "\n",
        "Poor performance in high dimensions.\n",
        "\n",
        "Doesn’t work well with missing values.\n",
        "\n",
        "\n",
        "9. How does feature scaling affect KNN?\n",
        "\n",
        "Feature scaling (e.g., normalization or standardization) is critical for KNN. Since it relies on distance, features with larger scales can dominate the distance metric and skew results.\n",
        "\n",
        "10. What is PCA (Principal Component Analysis)?\n",
        "\n",
        "PCA is a dimensionality reduction technique that transforms the original features into a smaller set of uncorrelated variables (principal components), capturing the maximum variance.\n",
        "\n",
        "11. How does PCA work?\n",
        "\n",
        "Standardize the data.\n",
        "\n",
        "Compute the covariance matrix.\n",
        "\n",
        "Find eigenvectors and eigenvalues.\n",
        "\n",
        "Sort eigenvectors by descending eigenvalues.\n",
        "\n",
        "Select the top k eigenvectors to form the new feature space.\n",
        "\n",
        "\n",
        "\n",
        "12. What is the geometric intuition behind PCA?\n",
        "\n",
        "PCA finds the directions (axes) in the feature space where the data varies the most and projects the data onto those directions to reduce dimensions while retaining maximum variance.\n",
        "\n",
        "13. What is the difference between Feature Selection and Feature Extraction?\n",
        "\n",
        "Feature Selection: Selects a subset of original features (e.g., removing irrelevant features).\n",
        "Feature Extraction: Transforms data into a new feature space (e.g., PCA).\n",
        "\n",
        "14. What are Eigenvalues and Eigenvectors in PCA?\n",
        "\n",
        "Eigenvectors represent the directions of maximum variance (principal components).\n",
        "Eigenvalues represent the magnitude of variance in each direction (importance of each component).\n",
        "\n",
        "15. How do you decide the number of components to keep in PCA?\n",
        "\n",
        "Use:\n",
        "Scree plot (elbow method)\n",
        "Cumulative explained variance (e.g., choose components that explain 95% of the variance)\n",
        "\n",
        "16. Can PCA be used for classification?\n",
        "\n",
        "Yes, PCA can be used as a preprocessing step to reduce dimensionality and improve classification performance. But PCA itself is not a classification algorithm.\n",
        "\n",
        "17. What are the limitations of PCA?\n",
        "\n",
        "Assumes linear relationships.\n",
        "\n",
        "Components may not be interpretable.\n",
        "\n",
        "Sensitive to scaling.\n",
        "\n",
        "Doesn’t consider class labels (unsupervised).\n",
        "\n",
        "\n",
        "18. How do KNN and PCA complement each other?\n",
        "\n",
        "PCA reduces dimensions and noise, which helps improve KNN performance by mitigating the curse of dimensionality and making distance metrics more meaningful.\n",
        "\n",
        "19. How does KNN handle missing values in a dataset?\n",
        "\n",
        "KNN doesn’t inherently handle missing values. You need to preprocess the data:\n",
        "Impute missing values (mean, median, or using KNN imputation).\n",
        "Remove rows/columns with missing data.\n",
        "\n",
        "\n",
        "20. What are the key differences between PCA and Linear Discriminant Analysis (LDA)?\n",
        "\n",
        "PCA is unsupervised, LDA is supervised.\n",
        "PCA maximizes variance, LDA maximizes class separability.\n",
        "PCA does not use class labels, LDA requires class labels.\n",
        "PCA creates principal components, LDA creates discriminant components.\n",
        "PCA captures global structure, LDA focuses on between-class vs. within-class variance.\n",
        "PCA is suitable for feature extraction, LDA is suitable for classification.\n",
        "PCA can be used with or without labels, LDA cannot work without labels.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "43ZSmvVt1Kcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical questions"
      ],
      "metadata": {
        "id": "w_bN6pvS1mGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Train a KNN Classifier on the Iris dataset and print model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xpa81aLT1mWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pIdDBgYw1muw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE)\n",
        "\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = make_regression(n_samples=200, n_features=1, noise=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
        "knn_reg.fit(X_train, y_train)\n",
        "y_pred = knn_reg.predict(X_test)\n",
        "\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "0cVjhLYy137e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Train a KNN Classifier using different distance metrics and compare accuracy\n",
        "\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=3, metric='manhattan')\n",
        "\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "\n",
        "acc_euclidean = accuracy_score(y_test, knn_euclidean.predict(X_test))\n",
        "acc_manhattan = accuracy_score(y_test, knn_manhattan.predict(X_test))\n",
        "\n",
        "print(\"Euclidean Accuracy:\", acc_euclidean)\n",
        "print(\"Manhattan Accuracy:\", acc_manhattan)\n"
      ],
      "metadata": {
        "id": "Jqg7Kyj017Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Train a KNN Classifier with different values of K and visualize decision boundaries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "X, y = make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
        "\n",
        "for k in [1, 3, 5, 7]:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X, y)\n",
        "    plt.figure()\n",
        "    plot_decision_regions(X, y, clf=knn, legend=2)\n",
        "    plt.title(f\"K = {k}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "K58kK1wi1-Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Apply Feature Scaling before training a KNN model and compare results\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Without scaling\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "acc_unscaled = accuracy_score(y_test, knn_unscaled.predict(X_test))\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "acc_scaled = accuracy_score(y_test, knn_scaled.predict(X_test_scaled))\n",
        "\n",
        "print(\"Unscaled Accuracy:\", acc_unscaled)\n",
        "print(\"Scaled Accuracy:\", acc_scaled)\n"
      ],
      "metadata": {
        "id": "iBoNzVyU2BDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26. Train a PCA model on synthetic data and print explained variance ratio\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "X, _ = make_classification(n_samples=200, n_features=5, random_state=42)\n",
        "pca = PCA(n_components=5)\n",
        "pca.fit(X)\n",
        "\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "id": "VHLa_kV22ELC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27. Apply PCA before training a KNN Classifier and compare accuracy\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "acc_pca = accuracy_score(y_test, knn_pca.predict(X_test_pca))\n",
        "\n",
        "print(\"Accuracy without PCA:\", acc_scaled)\n",
        "print(\"Accuracy with PCA:\", acc_pca)\n"
      ],
      "metadata": {
        "id": "hTM570bx2IxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_neighbors': [1, 3, 5, 7],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Score:\", grid.best_score_)\n"
      ],
      "metadata": {
        "id": "-ANXNJcw2L9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29. Train a KNN Classifier and check the number of misclassified samples\n",
        "\n",
        "y_pred = knn_scaled.predict(X_test_scaled)\n",
        "misclassified = (y_test != y_pred).sum()\n",
        "\n",
        "print(\"Number of Misclassified Samples:\", misclassified)\n"
      ],
      "metadata": {
        "id": "kyV8zyHs2O7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30. Train a PCA model and visualize the cumulative explained variance\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_train_scaled)\n",
        "\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "plt.plot(range(1, len(cumulative_variance)+1), cumulative_variance, marker='o')\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.title(\"PCA Cumulative Explained Variance\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z8V1vEgr2STA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#31. Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare accuracy.\n",
        "knn_uniform = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
        "knn_distance = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
        "\n",
        "knn_uniform.fit(X_train_scaled, y_train)\n",
        "knn_distance.fit(X_train_scaled, y_train)\n",
        "\n",
        "acc_uniform = accuracy_score(y_test, knn_uniform.predict(X_test_scaled))\n",
        "acc_distance = accuracy_score(y_test, knn_distance.predict(X_test_scaled))\n",
        "\n",
        "print(\"Uniform Weights Accuracy:\", acc_uniform)\n",
        "print(\"Distance Weights Accuracy:\", acc_distance)\n"
      ],
      "metadata": {
        "id": "QgqqkN132T4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#32. Train a KNN Regressor and analyze the effect of different K values on performance.\n",
        "for k in [1, 3, 5, 7, 9]:\n",
        "    model = KNeighborsRegressor(n_neighbors=k)\n",
        "    model.fit(X_train, y_train)\n",
        "    mse = mean_squared_error(y_test, model.predict(X_test))\n",
        "    print(f\"K={k}, MSE={mse}\")\n"
      ],
      "metadata": {
        "id": "O0cOVneV2eM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#33. Implement KNN Imputation for handling missing values in a dataset.\n",
        "from sklearn.impute import KNNImputer\n",
        "import numpy as np\n",
        "\n",
        "X_missing = X.copy()\n",
        "X_missing[::10] = np.nan  # introduce missing values\n",
        "imputer = KNNImputer(n_neighbors=3)\n",
        "X_imputed = imputer.fit_transform(X_missing)\n",
        "\n",
        "print(\"Missing values imputed using KNN.\")\n"
      ],
      "metadata": {
        "id": "3-CuoiRp2gZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#34. Train a PCA model and visualize the data projection onto the first two principal components.\n",
        "X_pca = PCA(n_components=2).fit_transform(X_scaled)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
        "plt.title(\"PCA Projection (First 2 Components)\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rnQMfgOs2iJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#35. Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance.\n",
        "for algo in ['kd_tree', 'ball_tree']:\n",
        "    knn = KNeighborsClassifier(algorithm=algo)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    acc = accuracy_score(y_test, knn.predict(X_test_scaled))\n",
        "    print(f\"{algo.upper()} Accuracy: {acc}\")\n"
      ],
      "metadata": {
        "id": "jTC5MP_Z2kEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#36. Train a PCA model on a high-dimensional dataset and visualize the Scree plot.\n",
        "X_hd = make_classification(n_samples=500, n_features=50)[0]\n",
        "pca_hd = PCA().fit(X_hd)\n",
        "plt.plot(np.cumsum(pca_hd.explained_variance_ratio_), marker='o')\n",
        "plt.title(\"Scree Plot\")\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_rF4y8fP2nax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#37. Train a KNN Classifier and evaluate performance using Precision, Recall, and Fl-Score.\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "z5_I3ZA22pI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#38. Train a PCA model and analyze the effect of different numbers of components on accuracy.\n",
        "for n in [1, 2, 3, 4]:\n",
        "    X_train_pca = PCA(n_components=n).fit_transform(X_train_scaled)\n",
        "    X_test_pca = PCA(n_components=n).fit(X_train_scaled).transform(X_test_scaled)\n",
        "    knn = KNeighborsClassifier()\n",
        "    knn.fit(X_train_pca, y_train)\n",
        "    print(f\"Components: {n}, Accuracy: {accuracy_score(y_test, knn.predict(X_test_pca))}\")\n"
      ],
      "metadata": {
        "id": "Cl2_fzEh2qyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#39. Train a KNN Classifier with different leaf_size values and compare accuracy.\n",
        "for size in [10, 20, 30, 50]:\n",
        "    knn = KNeighborsClassifier(leaf_size=size)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    print(f\"Leaf Size: {size}, Accuracy: {accuracy_score(y_test, knn.predict(X_test_scaled))}\")\n"
      ],
      "metadata": {
        "id": "SXv414l_2sZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#40. Train a PCA model and visualize how data points are transformed before and after PCA.\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "ax1.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y)\n",
        "ax1.set_title(\"Original Features\")\n",
        "X_pca = PCA(n_components=2).fit_transform(X_scaled)\n",
        "ax2.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
        "ax2.set_title(\"PCA Transformed\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2HeynL42vZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#41. Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report.\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "print(classification_report(y_test, knn.predict(X_test_scaled)))\n"
      ],
      "metadata": {
        "id": "Rn3uWAJ52w-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#42. Train a KNN Regressor and analyze the effect of different distance metrics on prediction error.\n",
        "for metric in ['euclidean', 'manhattan']:\n",
        "    knn = KNeighborsRegressor(n_neighbors=5, metric=metric)\n",
        "    knn.fit(X_train, y_train)\n",
        "    mse = mean_squared_error(y_test, knn.predict(X_test))\n",
        "    print(f\"{metric.capitalize()} MSE: {mse}\")\n"
      ],
      "metadata": {
        "id": "M5J16u5T2y_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#43. Train a KNN Classifier and evaluate using ROC-AUC score.\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "y_bin = label_binarize(y_test, classes=np.unique(y))\n",
        "y_score = knn.predict_proba(X_test_scaled)\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_bin, y_score, multi_class='ovr'))\n"
      ],
      "metadata": {
        "id": "MzxP_ydo20NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#44. Train a PCA model and visualize the variance captured by each principal component.\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "plt.bar(range(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)\n",
        "plt.title(\"Variance by Principal Component\")\n",
        "plt.xlabel(\"Component\")\n",
        "plt.ylabel(\"Variance Ratio\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZGRyd-m_21fQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#45. Train a KNN Classifier and perform feature selection before training.\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "selector = SelectKBest(score_func=f_classif, k=2)\n",
        "X_selected = selector.fit_transform(X_scaled, y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2)\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy with Selected Features:\", accuracy_score(y_test, knn.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "RYKk2z8d2203"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#46. Train a PCA model and visualize the data reconstruction error after reducing dimensions.\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "X_reconstructed = pca.inverse_transform(X_pca)\n",
        "reconstruction_error = np.mean((X_scaled - X_reconstructed)**2)\n",
        "print(\"Reconstruction Error:\", reconstruction_error)\n"
      ],
      "metadata": {
        "id": "ul6x0Zv_235q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#47. Train a KNN Classifier and visualize the decision boundary.\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "X_2d, y_2d = X_scaled[:, :2], y\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_2d, y_2d)\n",
        "\n",
        "plt.figure()\n",
        "plot_decision_regions(X_2d, y_2d, clf=knn, legend=2)\n",
        "plt.title(\"KNN Decision Boundary\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qUeqBuKw25Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#48. Train a PCA model and analyze the effect of different numbers of components on data variance.\n",
        "pca = PCA().fit(X_scaled)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.title(\"Cumulative Variance by PCA Components\")\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Cumulative Variance\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nHjLbv8d26Eb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}