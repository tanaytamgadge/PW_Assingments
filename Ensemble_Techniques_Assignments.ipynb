{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3lEAbXwasrS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J4kAa49nfCTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 1 - Theory**\n",
        "\n",
        "1. **Can we use Bagging for regression problems?**  \n",
        "   Yes, Bagging can be used for regression problems. A common example is the **Bagging Regressor**, which aggregates predictions from multiple base regressors (e.g., Decision Trees) and averages their outputs to improve stability and accuracy.  \n",
        "\n",
        "2. **What is the difference between multiple model training and single model training?**  \n",
        "   - **Single Model Training**: A single model learns from the data and makes predictions independently. It can be prone to overfitting or underfitting.  \n",
        "   - **Multiple Model Training (Ensemble Learning)**: Multiple models are trained on the data and their predictions are combined (e.g., averaging, voting) to improve performance, robustness, and generalization.  \n",
        "\n",
        "3. **Explain the concept of feature randomness in Random Forest.**  \n",
        "   In Random Forest, feature randomness refers to selecting a **random subset of features** at each split in a decision tree. This introduces diversity in the trees and helps reduce overfitting while improving generalization.  \n",
        "\n",
        "4. **What is OOB (Out-of-Bag) Score?**  \n",
        "   OOB Score is a performance estimate calculated using the samples that were **not included in the bootstrap sample** for training each tree in a Bagging model. It provides an unbiased measure of model accuracy.  \n",
        "\n",
        "5. **How can you measure the importance of features in a Random Forest model?**  \n",
        "   Feature importance can be measured using:  \n",
        "   1. **Mean Decrease in Impurity (Gini Importance)** – How much a feature reduces impurity across all splits.  \n",
        "   2. **Permutation Importance** – The decrease in model performance when a feature’s values are randomly shuffled.  \n",
        "\n",
        "6. **Explain the working principle of a Bagging Classifier.**  \n",
        "   A Bagging Classifier works as follows:  \n",
        "   1. Create multiple subsets of the training data using **bootstrap sampling**.  \n",
        "   2. Train a separate model (e.g., Decision Tree) on each subset.  \n",
        "   3. Aggregate predictions using **majority voting** (for classification).  \n",
        "\n",
        "7. **How do you evaluate a Bagging Classifier's performance?**  \n",
        "   A Bagging Classifier's performance can be evaluated using:  \n",
        "   - **Accuracy** (for classification)  \n",
        "   - **Confusion Matrix, Precision, Recall, F1-score**  \n",
        "   - **ROC-AUC Score** (for binary classification)  \n",
        "   - **Cross-validation** to ensure robustness  \n",
        "\n",
        "8. **How does a Bagging Regressor work?**  \n",
        "   A Bagging Regressor works by:  \n",
        "   1. Generating multiple bootstrap samples from the dataset.  \n",
        "   2. Training individual regressors (e.g., Decision Trees) on each subset.  \n",
        "   3. Averaging the predictions from all regressors to reduce variance and improve accuracy.  \n",
        "\n",
        "9. **What is the main advantage of ensemble techniques?**  \n",
        "   Ensemble techniques improve **accuracy, stability, and robustness** by combining multiple models, reducing overfitting and variance.  \n",
        "\n",
        "10. **What is the main challenge of ensemble methods?**  \n",
        "    The main challenges include:  \n",
        "    - **Computational cost** (training multiple models)  \n",
        "    - **Complexity** (harder to interpret than single models)  \n",
        "    - **Risk of overfitting** if not tuned properly  \n",
        "\n",
        "11. **Explain the key idea behind ensemble techniques.**  \n",
        "    The key idea is to combine multiple weak models to create a **stronger model** that performs better than any individual model alone.  \n",
        "\n",
        "12. **What is a Random Forest Classifier?**  \n",
        "    A **Random Forest Classifier** is an ensemble method that builds multiple Decision Trees and aggregates their predictions using **majority voting**. It introduces randomness in feature selection and bootstrap sampling to improve generalization.  \n",
        "\n",
        "13. **What are the main types of ensemble techniques?**  \n",
        "    1. **Bagging** – Reduces variance (e.g., Random Forest).  \n",
        "    2. **Boosting** – Reduces bias (e.g., AdaBoost, XGBoost).  \n",
        "    3. **Stacking** – Combines multiple models using a meta-learner.  \n",
        "    4. **Voting/Averaging** – Combines predictions from multiple models.  \n",
        "\n",
        "14. **What is ensemble learning in machine learning?**  \n",
        "    Ensemble learning is a technique where multiple models are combined to improve overall accuracy, reduce overfitting, and enhance robustness.  \n",
        "\n",
        "15. **When should we avoid using ensemble methods?**  \n",
        "    - When interpretability is critical (e.g., medical applications).  \n",
        "    - When a single simple model performs well enough.  \n",
        "    - When computational resources are limited.  \n",
        "\n",
        "16. **How does Bagging help in reducing overfitting?**  \n",
        "    Bagging reduces overfitting by training multiple models on different random subsets of data and averaging their predictions, which reduces variance.  \n",
        "\n",
        "17. **Why is Random Forest better than a single Decision Tree?**  \n",
        "    - **Less Overfitting** – Averaging multiple trees improves generalization.  \n",
        "    - **Higher Accuracy** – More stable and robust predictions.  \n",
        "    - **Handles Missing Data & Feature Importance** better.  \n",
        "\n",
        "18. **What is the role of bootstrap sampling in Bagging?**  \n",
        "    Bootstrap sampling creates diverse training subsets by randomly selecting samples **with replacement**, ensuring different trees learn different patterns and reduce overfitting.  \n",
        "\n",
        "19. **What are some real-world applications of ensemble techniques?**  \n",
        "    - **Finance** (fraud detection, risk assessment)  \n",
        "    - **Healthcare** (disease prediction, medical diagnosis)  \n",
        "    - **E-commerce** (recommendation systems)  \n",
        "    - **Image Recognition** (object detection, facial recognition)  \n",
        "\n",
        "20. **What is the difference between Bagging and Boosting?**  \n",
        "\n",
        "| Feature  | Bagging | Boosting |  \n",
        "|----------|---------|---------|  \n",
        "| Goal  | Reduce variance  | Reduce bias & variance |  \n",
        "| How it Works | Trains multiple models independently and averages results | Trains models sequentially, giving more weight to misclassified samples |  \n",
        "| Overfitting | Less prone | More prone if not tuned properly |  \n",
        "| Example Algorithms | Random Forest, Bagging Classifier | AdaBoost, Gradient Boosting, XGBoost |  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9qSyD-Mtat6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical Questions\n"
      ],
      "metadata": {
        "id": "ulZJSV6fa0td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix, roc_auc_score, precision_recall_curve, classification_report,\n",
        "\n",
        "precision_score, recall_score, f1_score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer, make_regression\n",
        "\n",
        "# Load Breast Cancer dataset for classification\n",
        "data = load_breast_cancer()\n",
        "\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load dataset for regression\n",
        "\n",
        "X_reg, y_reg = make_regression(n_samples=500, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# 31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, None]}\n",
        "\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 32. Train a Bagging Regressor with different base estimators and compare performance\n",
        "for n in [10, 50, 100]:\n",
        "    bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=n, random_state=42)\n",
        "    \n",
        "    bagging_reg.fit(X_train_reg, y_train_reg)\n",
        "    \n",
        "    y_pred_reg = bagging_reg.predict(X_test_reg)\n",
        "    \n",
        "    print(f\"Bagging Regressor ({n} estimators) MSE:\", mean_squared_error(y_test_reg, y_pred_reg))\n",
        "\n",
        "# 33. Train a Random Forest Classifier and analyze misclassified samples\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "misclassified = X_test[y_test != rf_clf.predict(X_test)]\n",
        "\n",
        "print(\"Misclassified Samples Count:\", len(misclassified))\n",
        "\n",
        "# 34. Compare Bagging Classifier with Decision Tree Classifier\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "\n",
        "dt_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
        "\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy_score(y_test, y_pred_bag))\n",
        "\n",
        "# 35. Train a Random Forest Classifier and visualize confusion matrix\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_rf)\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d')\n",
        "\n",
        "plt.xlabel('Predicted')\n",
        "\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# 36. Train a Stacking Classifier and compare accuracy\n",
        "stacking_clf = StackingClassifier(estimators=[('dt', DecisionTreeClassifier()), ('svm', SVC(probability=True)), ('lr', LogisticRegression())],\n",
        "\n",
        "final_estimator=LogisticRegression())\n",
        "\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_stack = stacking_clf.predict(X_test)\n",
        "\n",
        "print(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, y_pred_stack))\n",
        "\n",
        "# 37. Print the top 5 most important features from Random Forest\n",
        "feature_importances = pd.Series(rf_clf.feature_importances_, index=data.\n",
        "feature_names).nlargest(5)\n",
        "\n",
        "print(\"Top 5 Features:\", feature_importances)\n",
        "\n",
        "# 38. Evaluate Bagging Classifier using Precision, Recall, and F1-score\n",
        "print(\"Classification Report for Bagging Classifier:\")\n",
        "\n",
        "print(classification_report(y_test, y_pred_bag))\n",
        "\n",
        "# 39. Analyze effect of max_depth on accuracy\n",
        "for depth in [5, 10, None]:\n",
        "    rf_depth = RandomForestClassifier(max_depth=depth, random_state=42)\n",
        "    \n",
        "    rf_depth.fit(X_train, y_train)\n",
        "    \n",
        "    print(f\"Max Depth {depth} Accuracy:\", accuracy_score(y_test, rf_depth.predict(X_test)))\n",
        "\n",
        "# 40. Compare Bagging Regressor with different base estimators\n",
        "dt_bagging = BaggingRegressor(base_estimator=DecisionTreeRegressor(), random_state=42)\n",
        "\n",
        "knn_bagging = BaggingRegressor(base_estimator=KNeighborsRegressor(), random_state=42)\n",
        "\n",
        "dt_bagging.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "knn_bagging.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "print(\"DT Bagging MSE:\", mean_squared_error(y_test_reg, dt_bagging.predict(X_test_reg)))\n",
        "\n",
        "print(\"KNN Bagging MSE:\", mean_squared_error(y_test_reg, knn_bagging.predict(X_test_reg)))\n",
        "\n",
        "# 41. Evaluate Random Forest Classifier using ROC-AUC Score\n",
        "y_pred_proba_rf = rf_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred_proba_rf))\n",
        "\n",
        "# 42. Evaluate Bagging Classifier using Cross-Validation\n",
        "cv_scores = cross_val_score(bagging_clf, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "\n",
        "# 43. Plot Precision-Recall curve for Random Forest\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba_rf)\n",
        "\n",
        "plt.plot(recall, precision)\n",
        "\n",
        "plt.xlabel(\"Recall\")\n",
        "\n",
        "plt.ylabel(\"Precision\")\n",
        "\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# 44. Train a Stacking Classifier with Random Forest and Logistic Regression\n",
        "stacking_clf2 = StackingClassifier(estimators=[('rf', RandomForestClassifier()), ('lr', LogisticRegression())], final_estimator=LogisticRegression())\n",
        "\n",
        "stacking_clf2.fit(X_train, y_train)\n",
        "\n",
        "y_pred_stack2 = stacking_clf2.predict(X_test)\n",
        "\n",
        "print(\"Stacking Classifier (RF+LR) Accuracy:\", accuracy_score(y_test,\n",
        "y_pred_stack2))\n",
        "\n",
        "# 45. Compare Bagging Regressor with different bootstrap sample levels\n",
        "for bootstrap_val in [True, False]:\n",
        "    \n",
        "    bagging_reg_bs = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
        "    bootstrap=bootstrap_val, random_state=42)\n",
        "    \n",
        "    bagging_reg_bs.fit(X_train_reg, y_train_reg)\n",
        "    \n",
        "    print(f\"Bootstrap {bootstrap_val} MSE:\", mean_squared_error(y_test_reg, bagging_reg_bs.predict(X_test_reg)))\n"
      ],
      "metadata": {
        "id": "I2iBlvo8fEIt"
      }
    }
  ]
}