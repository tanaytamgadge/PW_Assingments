{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfPi4USWZlsG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theory Questions\n",
        "1. What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that best separates different classes in the dataset. SVM aims to maximize the margin between the closest data points (support vectors) from different classes.\n",
        "\n",
        "\n",
        "2. What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "Hard Margin SVM: Strictly separates data points without allowing misclassification. Suitable for linearly separable data but sensitive to noise.\n",
        "Soft Margin SVM: Allows some misclassification to handle overlapping or non-linearly separable data, using a penalty parameter (C) to balance margin maximization and classification errors.\n",
        "\n",
        "\n",
        "3. What is the mathematical intuition behind SVM?\n",
        "\n",
        "SVM finds the hyperplane that maximizes the margin between two classes. Mathematically, it solves the following optimization problem:\n",
        "min⁡w,b12∣∣w∣∣2\\min_{w, b} \\frac{1}{2} ||w||^2\n",
        "Subject to:\n",
        "yi(w⋅xi+b)≥1∀iy_i (w \\cdot x_i + b) \\geq 1 \\quad \\forall i\n",
        "For non-linearly separable data, a slack variable ξ\\xi is introduced:\n",
        "yi(w⋅xi+b)≥1−ξiy_i (w \\cdot x_i + b) \\geq 1 - \\xi_i\n",
        "\n",
        "4. What is the role of Lagrange Multipliers in SVM?\n",
        "\n",
        "Lagrange multipliers help solve the constrained optimization problem in SVM by transforming it into a dual form. This allows SVM to efficiently find the optimal hyperplane using kernel functions.\n",
        "\n",
        "\n",
        "\n",
        "5. What are Support Vectors in SVM?\n",
        "\n",
        "Support vectors are the data points that lie closest to the decision boundary (hyperplane). They define the margin and influence the position of the hyperplane.\n",
        "\n",
        "\n",
        "6. What is a Support Vector Classifier (SVC)?\n",
        "\n",
        "SVC is an extension of SVM used for classification tasks, allowing soft margins to handle noisy and overlapping data.\n",
        "\n",
        "\n",
        "7. What is a Support Vector Regressor (SVR)?\n",
        "\n",
        "SVR applies the SVM concept to regression problems. It finds a function that approximates the data while allowing a margin of tolerance (epsilon ε\\varepsilon).\n",
        "\n",
        "\n",
        "8. What is the Kernel Trick in SVM?\n",
        "\n",
        "The kernel trick enables SVM to handle non-linearly separable data by mapping input features into a higher-dimensional space where a linear separator exists.\n",
        "\n",
        "\n",
        "9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel\n",
        "\n",
        "Linear Kernel: Suitable for linearly separable data.\n",
        "Polynomial Kernel: Captures non-linear relationships with polynomial degrees.\n",
        "RBF Kernel: Uses Gaussian function to model complex decision boundaries and is widely used for non-linear data.\n",
        "\n",
        "10. What is the effect of the C parameter in SVM?\n",
        "\n",
        "The parameter C controls the trade-off between maximizing margin and minimizing misclassification. Higher C values reduce margin but decrease misclassification, while lower C values allow a larger margin with potential misclassification.\n",
        "\n",
        "11. What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "\n",
        "Gamma determines how far the influence of a training example reaches. Higher gamma values lead to more complex models (overfitting), while lower values create simpler models (underfitting).\n",
        "\n",
        "\n",
        "12. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "Naïve Bayes is a probabilistic classifier based on Bayes' Theorem, assuming independence among features. It is called \"naïve\" because it assumes that all features contribute independently to the probability of a class, which is rarely true in real-world scenarios.\n",
        "\n",
        "\n",
        "13. What is Bayes' Theorem?\n",
        "\n",
        "Bayes' Theorem describes the probability of an event based on prior knowledge:\n",
        "P(A∣B)=P(B∣A)P(A)P(B)P(A|B) = \\frac{P(B|A) P(A)}{P(B)}\n",
        "where:\n",
        "P(A∣B)P(A|B) is the posterior probability.\n",
        "P(B∣A)P(B|A) is the likelihood.\n",
        "P(A)P(A) is the prior probability.\n",
        "P(B)P(B) is the marginal probability.\n",
        "\n",
        "14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "Gaussian Naïve Bayes: Used for continuous data assuming normal distribution.\n",
        "Multinomial Naïve Bayes: Used for discrete data like word counts in text classification.\n",
        "Bernoulli Naïve Bayes: Used for binary features (presence/absence of words).\n",
        "\n",
        "15. When should you use Gaussian Naïve Bayes over other variants?\n",
        "\n",
        "Gaussian Naïve Bayes is preferable when dealing with continuous numerical data that follows a normal distribution, such as height, weight, or sensor data.\n",
        "\n",
        "\n",
        "16. What are the key assumptions made by Naïve Bayes?\n",
        "\n",
        "Features are independent given the class.\n",
        "Each feature contributes equally to the outcome.\n",
        "Data follows a specific distribution (e.g., Gaussian for continuous data).\n",
        "\n",
        "17. What are the advantages and disadvantages of Naïve Bayes?\n",
        "\n",
        "Advantages:\n",
        "Fast and efficient for large datasets.\n",
        "Works well with high-dimensional data.\n",
        "Performs well in text classification tasks.\n",
        "Disadvantages:\n",
        "Assumes feature independence, which may not hold.\n",
        "Struggles with correlated features.\n",
        "Requires sufficient data to estimate probabilities accurately.\n",
        "\n",
        "\n",
        "18. Why is Naïve Bayes a good choice for text classification?\n",
        "\n",
        "Naïve Bayes performs well in text classification because:\n",
        "It handles high-dimensional data efficiently.\n",
        "It is robust to irrelevant features.\n",
        "It requires minimal training data.\n",
        "\n",
        "19. Compare SVM and Naïve Bayes for classification tasks\n",
        "\n",
        "SVM is a margin-based classifier that works well for complex, high-dimensional data but is computationally expensive. Naïve Bayes is a probabilistic classifier that is fast and effective, especially for text classification, but struggles with correlated features. SVM excels in structured datasets, while Naïve Bayes is ideal for quick, efficient classification tasks.\n",
        "\n",
        "20. How does Laplace Smoothing help in Naïve Bayes?\n",
        "\n",
        "Laplace Smoothing (also called additive smoothing) prevents zero probabilities by adding a small constant to all probability estimates. This ensures unseen words in the test data do not lead to a probability of zero.\n",
        "P(w∣c)=(count(w,c)+α)(count(c)+α×∣V∣)P(w|c) = \\frac{(count(w,c) + \\alpha)}{(count(c) + \\alpha \\times |V|)}\n",
        "where α\\alpha is a smoothing parameter, typically set to 1.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CgKYm3MrZmpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical questions"
      ],
      "metadata": {
        "id": "47S7heoucZwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.\n",
        "from sklearn import datasets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM classifier\n",
        "svm_clf = SVC(kernel='linear')\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"SVM Classifier Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "O9bYt4Y_ccVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = datasets.load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "\n",
        "# Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Compare Accuracies\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"Linear Kernel Accuracy: {accuracy_linear}\")\n",
        "print(f\"RBF Kernel Accuracy: {accuracy_rbf}\")\n"
      ],
      "metadata": {
        "id": "0gh5tuk0dly4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "boston = datasets.load_diabetes()\n",
        "X, y = boston.data, boston.target  # Using Diabetes dataset as a housing dataset alternative\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVR model\n",
        "svr = SVR(kernel='rbf')\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = svr.predict(X_test)\n",
        "\n",
        "# Evaluate MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ],
      "metadata": {
        "id": "ex1Ck2UpdtXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate dataset\n",
        "X, y = make_moons(n_samples=100, noise=0.1, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train SVM with Polynomial Kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1)\n",
        "svm_poly.fit(X_scaled, y)\n",
        "\n",
        "# Plot decision boundary\n",
        "xx, yy = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
        "\n",
        "Z = svm_poly.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, edgecolors='k')\n",
        "\n",
        "plt.title(\"SVM with Polynomial Kernel\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UIgC5spSd4PJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.\n",
        "from sklearn import datasets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Gaussian Naïve Bayes Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "9yoyi7_QeBaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer,\n",
        "\n",
        "TfidfTransformer\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=['sci.space', 'rec.autos', 'comp.graphics'])\n",
        "\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=['sci.space', 'rec.autos', 'comp.graphics'])\n",
        "\n",
        "# Create a pipeline for text classification\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),  \n",
        "    ('tfidf', TfidfTransformer()),  \n",
        "    ('clf', MultinomialNB()),  \n",
        "])\n",
        "\n",
        "# Train model\n",
        "text_clf.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "\n",
        "# Predict\n",
        "y_pred = text_clf.predict(newsgroups_test.data)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(newsgroups_test.target, y_pred)\n",
        "\n",
        "print(\"Multinomial Naïve Bayes Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "yQYng_YBeBhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27.\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate dataset\n",
        "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Define different C values\n",
        "C_values = [0.1, 1, 10]\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i, C in enumerate(C_values, 1):\n",
        "    svm = SVC(kernel='linear', C=C)\n",
        "    svm.fit(X_scaled, y)\n",
        "\n",
        "    # Plot decision boundary\n",
        "    xx, yy = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
        "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.subplot(1, 3, i)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, edgecolors='k')\n",
        "    plt.title(f\"SVM Decision Boundary (C={C})\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7te8ltcfeBjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28.\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate binary feature dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "X = (X > 0).astype(int)  # Convert features to binary\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bernoulli Naïve Bayes\n",
        "bnb = BernoulliNB()\n",
        "\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = bnb.predict(X_test)\n",
        "\n",
        "print(\"Bernoulli Naïve Bayes Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "jAc9ASQ0eBl_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29.\n",
        "from sklearn import datasets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = datasets.load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM without scaling\n",
        "svm_no_scaling = SVC(kernel='rbf')\n",
        "svm_no_scaling.fit(X_train, y_train)\n",
        "accuracy_no_scaling = accuracy_score(y_test, svm_no_scaling.predict(X_test))\n",
        "\n",
        "# Apply Standard Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM with scaling\n",
        "svm_with_scaling = SVC(kernel='rbf')\n",
        "svm_with_scaling.fit(X_train_scaled, y_train)\n",
        "accuracy_with_scaling = accuracy_score(y_test, svm_with_scaling.predict(X_test_scaled))\n",
        "\n",
        "print(f\"Without Scaling Accuracy: {accuracy_no_scaling}\")\n",
        "\n",
        "print(f\"With Scaling Accuracy: {accuracy_with_scaling}\")\n"
      ],
      "metadata": {
        "id": "GmzHh2_heBop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model without Laplace smoothing\n",
        "gnb_no_smoothing = GaussianNB(var_smoothing=1e-9)\n",
        "gnb_no_smoothing.fit(X_train, y_train)\n",
        "\n",
        "# Train model with Laplace smoothing\n",
        "gnb_smoothing = GaussianNB(var_smoothing=1e-2)\n",
        "gnb_smoothing.fit(X_train, y_train)\n",
        "\n",
        "# Compare accuracy\n",
        "print(\"Without Smoothing Accuracy:\", accuracy_score(y_test, gnb_no_smoothing.predict(X_test)))\n",
        "\n",
        "print(\"With Smoothing Accuracy:\", accuracy_score(y_test, gnb_smoothing.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "vgroa7MDeBq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Define parameters grid\n",
        "param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['linear', 'rbf']}\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "6_4fZUTQeBtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "32.\n",
        "from sklearn.utils import compute_class_weight\n",
        "\n",
        "# Generate imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Train normal SVM\n",
        "svm_no_weight = SVC()\n",
        "svm_no_weight.fit(X_train, y_train)\n",
        "\n",
        "accuracy_no_weight = accuracy_score(y_test, svm_no_weight.predict(X_test))\n",
        "\n",
        "# Train SVM with class weighting\n",
        "svm_weighted = SVC(class_weight='balanced')\n",
        "\n",
        "svm_weighted.fit(X_train, y_train)\n",
        "\n",
        "accuracy_weighted = accuracy_score(y_test, svm_weighted.predict(X_test))\n",
        "\n",
        "print(f\"Without Class Weighting Accuracy: {accuracy_no_weight}\")\n",
        "\n",
        "print(f\"With Class Weighting Accuracy: {accuracy_weighted}\")\n"
      ],
      "metadata": {
        "id": "ZS0wWZ-ueBv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "33.\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load spam dataset\n",
        "data = fetch_openml('sms_spam', version=1, as_frame=True)\n",
        "X, y = data.data['text'], data.target\n",
        "\n",
        "# Create pipeline\n",
        "spam_clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "\n",
        "# Train and evaluate\n",
        "spam_clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Spam Detection Accuracy:\", accuracy_score(y_test, spam_clf.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "scchaYBPeByc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "34.\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Train SVM\n",
        "svm = SVC(kernel='linear')\n",
        "\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "accuracy_svm = accuracy_score(y_test, svm.predict(X_test))\n",
        "\n",
        "# Train Naïve Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "accuracy_nb = accuracy_score(y_test, nb.predict(X_test))\n",
        "\n",
        "print(f\"SVM Accuracy: {accuracy_svm}\")\n",
        "print(f\"Naïve Bayes Accuracy: {accuracy_nb}\")\n"
      ],
      "metadata": {
        "id": "gdxHXKhfeB1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "35.\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Select best features\n",
        "selector = SelectKBest(chi2, k=10)\n",
        "X_new = selector.fit_transform(X, y)\n",
        "\n",
        "# Train Naïve Bayes with selected features\n",
        "nb_selected = GaussianNB()\n",
        "\n",
        "nb_selected.fit(X_train, y_train)\n",
        "\n",
        "accuracy_selected = accuracy_score(y_test, nb_selected.predict(X_test))\n",
        "\n",
        "print(\"Accuracy after Feature Selection:\", accuracy_selected)\n"
      ],
      "metadata": {
        "id": "i7b7LnSSeB3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "36.\n",
        "from sklearn import datasets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = datasets.load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train One-vs-Rest SVM\n",
        "ovr_svm = OneVsRestClassifier(SVC(kernel='linear'))\n",
        "ovr_svm.fit(X_train, y_train)\n",
        "accuracy_ovr = accuracy_score(y_test, ovr_svm.predict(X_test))\n",
        "\n",
        "# Train One-vs-One SVM\n",
        "ovo_svm = OneVsOneClassifier(SVC(kernel='linear'))\n",
        "ovo_svm.fit(X_train, y_train)\n",
        "accuracy_ovo = accuracy_score(y_test, ovo_svm.predict(X_test))\n",
        "\n",
        "print(f\"One-vs-Rest Accuracy: {accuracy_ovr}\")\n",
        "print(f\"One-vs-One Accuracy: {accuracy_ovo}\")\n"
      ],
      "metadata": {
        "id": "vRHbtQ57eB6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "37.\n",
        "from sklearn import datasets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train models with different kernels\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "for kernel in kernels:\n",
        "    svm = SVC(kernel=kernel)\n",
        "    \n",
        "    svm.fit(X_train, y_train)\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, svm.predict(X_test))\n",
        "    \n",
        "    print(f\"SVM with {kernel} kernel Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "M2-eVfNTeB9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "38.\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# Define stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Train and evaluate using cross-validation\n",
        "svm = SVC(kernel='linear')\n",
        "scores = cross_val_score(svm, X, y, cv=skf)\n",
        "\n",
        "print(f\"Average Accuracy using Stratified K-Fold: {scores.mean()}\")\n"
      ],
      "metadata": {
        "id": "hLLy-EJUeCAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "39.\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Train Naïve Bayes with different priors\n",
        "priors_list = [[0.7, 0.3], [0.5, 0.5], [0.3, 0.7]]\n",
        "\n",
        "for priors in priors_list:\n",
        "    nb = GaussianNB(priors=priors)\n",
        "    \n",
        "    nb.fit(X_train, y_train)\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, nb.predict(X_test))\n",
        "    \n",
        "    print(f\"Accuracy with priors {priors}: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "ZjVt3uNzeXVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "40.\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# Feature selection using RFE\n",
        "svm = SVC(kernel='linear')\n",
        "\n",
        "rfe = RFE(estimator=svm, n_features_to_select=10)\n",
        "\n",
        "X_rfe = rfe.fit_transform(X, y)\n",
        "\n",
        "# Train and evaluate SVM with selected features\n",
        "svm.fit(X_rfe, y)\n",
        "\n",
        "print(\"SVM Accuracy after RFE:\", accuracy_score(y_test, svm.predict(rfe.transform(X_test))))\n"
      ],
      "metadata": {
        "id": "uv_JpN_EeXjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "41.\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Train SVM\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "precision = precision_score(y_test, y_pred)\n",
        "\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n"
      ],
      "metadata": {
        "id": "HjFRTgJYeXmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "42.\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Train Naïve Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "y_prob = nb.predict_proba(X_test)\n",
        "\n",
        "# Compute Log Loss\n",
        "print(\"Log Loss:\", log_loss(y_test, y_prob))\n"
      ],
      "metadata": {
        "id": "4pA14lALeXow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "43.\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Train SVM\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "\n",
        "plt.xlabel(\"Predicted\")\n",
        "\n",
        "plt.ylabel(\"Actual\")\n",
        "\n",
        "plt.title(\"Confusion Matrix\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dJSiFBoCeXrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "44.\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load regression dataset\n",
        "diabetes = datasets.load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "\n",
        "# Train SVR\n",
        "svr = SVR(kernel='rbf')\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate MAE\n",
        "y_pred = svr.predict(X_test)\n",
        "print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "di2DAk3OeXvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "45.\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Train Naïve Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "y_prob = nb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob))\n"
      ],
      "metadata": {
        "id": "NYB1GstJefgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "46.\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Train SVM\n",
        "svm = SVC(probability=True)\n",
        "svm.fit(X_train, y_train)\n",
        "y_prob = svm.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute Precision-Recall Curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "x9z3A18Cegf-"
      }
    }
  ]
}