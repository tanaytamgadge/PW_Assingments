{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ketkDJd3ym2d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML assignment - theory\n",
        "\n",
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "Logistic Regression is a classification algorithm used for binary or multi-class classification problems. Unlike Linear Regression, which predicts continuous values, Logistic Regression predicts probabilities using the sigmoid function and applies a threshold to classify data into categories.\n"
      ],
      "metadata": {
        "id": "Pf8kZJCRytsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "The equation for Logistic Regression is:\n",
        "P(Y=1∣X)=11+e−(β0+β1X1+β2X2+...+βnXn)P(Y=1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n)}}\n",
        "where β0\\beta_0 is the intercept and β1,β2,...βn\\beta_1, \\beta_2, ... \\beta_n are coefficients."
      ],
      "metadata": {
        "id": "AX6FuT420goy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do we use the sigmoid function in Logistic Regression?\n",
        "\n",
        "The sigmoid function maps any real number to a value between 0 and 1, which makes it ideal for probability estimation. It helps in converting linear predictions into a probability score for classification."
      ],
      "metadata": {
        "id": "Mm22JXeZ0kkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the cost function of Logistic Regression?\n",
        "\n",
        "Logistic Regression uses the Log Loss (Binary Cross-Entropy) cost function:\n",
        "J(θ)=−1m∑i=1m[yilog⁡hθ(xi)+(1−yi)log⁡(1−hθ(xi))]J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log h_\\theta(x_i) + (1 - y_i) \\log (1 - h_\\theta(x_i)) \\right]\n",
        "where hθ(xi)h_\\theta(x_i) is the predicted probability."
      ],
      "metadata": {
        "id": "5fgL_UP70y-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "Regularization is used to prevent overfitting by adding a penalty term to the cost function. It discourages overly complex models by shrinking coefficient values."
      ],
      "metadata": {
        "id": "Aj8rBM0z02Z8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        "Lasso (L1 Regularization): Shrinks some coefficients to zero, performing feature selection.\n",
        "\n",
        "Ridge (L2 Regularization): Shrinks all coefficients but does not set them to zero.\n",
        "\n",
        "Elastic Net: A combination of L1 and L2 regularization, balancing both penalties."
      ],
      "metadata": {
        "id": "EgJlqZ2B0-Em"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "Elastic Net is preferred when there are many correlated features, as it selects groups of correlated variables rather than eliminating some entirely like Lasso."
      ],
      "metadata": {
        "id": "99qQsKV51Cxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "\n",
        "A high λ leads to high regularization, reducing overfitting but increasing bias.\n",
        "A low λ allows more flexibility but may cause overfitting."
      ],
      "metadata": {
        "id": "y3BO0E8j1HcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression?\n",
        "\n",
        "No multicollinearity among independent variables.\n",
        "Independent variables should be meaningful and relevant.\n",
        "Linearity between independent variables and the log-odds.\n",
        "No strong outliers affecting the model."
      ],
      "metadata": {
        "id": "uug3O2Rl1KNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "Decision Trees\n",
        "Random Forest\n",
        "Support Vector Machines (SVM)\n",
        "Naïve Bayes\n",
        "Neural Networks\n"
      ],
      "metadata": {
        "id": "TBAGFeOE1NnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are Classification Evaluation Metrics?\n",
        "\n",
        "Accuracy\n",
        "Precision\n",
        "Recall\n",
        "F1 Score\n",
        "ROC-AUC Curve\n"
      ],
      "metadata": {
        "id": "HObGZOGk1Q5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How does class imbalance affect Logistic Regression?\n",
        "\n",
        "In cases of class imbalance, Logistic Regression may predict the majority class more often, leading to misleading accuracy. Solutions include resampling, using weighted loss functions, or alternative evaluation metrics like precision-recall."
      ],
      "metadata": {
        "id": "P5fAM4sx1Uv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "Hyperparameter tuning involves optimizing model parameters such as the regularization strength (λ) and solver type to improve performance using techniques like Grid Search and Random Search."
      ],
      "metadata": {
        "id": "8CStJ5xH1anc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "lbfgs: Suitable for small to medium-sized datasets.\n",
        "saga: Works well with L1 and L2 regularization.\n",
        "liblinear: Good for small datasets with L1 or L2 regularization.\n",
        "newton-cg: Works well with L2 regularization but not L1.\n",
        "For large datasets, saga is a good choice."
      ],
      "metadata": {
        "id": "_CSwmxrM1esL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "Logistic Regression can be extended using:\n",
        "One-vs-Rest (OvR): Trains a separate classifier for each class against the rest.\n",
        "Softmax Regression (Multinomial Logistic Regression): Computes probabilities for all classes at once."
      ],
      "metadata": {
        "id": "sKxPMm5w1jFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "Advantages:\n",
        "Simple and interpretable.\n",
        "Works well with small datasets.\n",
        "Provides probabilistic interpretation.\n",
        "Disadvantages:\n",
        "Assumes linear relationship in log-odds.\n",
        "Sensitive to multicollinearity.\n",
        "Can struggle with complex, nonlinear relationships."
      ],
      "metadata": {
        "id": "n8C7dt8P1mV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are some use cases of Logistic Regression?\n",
        "\n",
        "Fraud detection\n",
        "Customer churn prediction\n",
        "Disease diagnosis (e.g., cancer detection)\n",
        "Spam email classification"
      ],
      "metadata": {
        "id": "20BhcKdY1qHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "Logistic Regression is used for binary classification.\n",
        "Softmax Regression generalizes Logistic Regression for multiclass classification by computing probabilities for multiple classes."
      ],
      "metadata": {
        "id": "NgM3jk7-1tEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "OvR: Suitable for sparse datasets or when training time is a concern.\n",
        "Softmax: Preferred when all classes are mutually exclusive, providing a more holistic probability distribution."
      ],
      "metadata": {
        "id": "Ogzkc7jO1xvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "Each coefficient represents the change in log-odds for a one-unit increase in the corresponding feature while keeping other features constant. The odds ratio can be calculated as eβe^\\beta, indicating how much more (or less) likely an event is with a given feature."
      ],
      "metadata": {
        "id": "-t2PUe7p10L-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sKTpnHC_1__I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML Assignment - Practical\n",
        "\n",
        "1.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (Example: Iris dataset)\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print model accuracy\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "WlZ09WTC2AGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (Example: Iris dataset)\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y\n"
      ],
      "metadata": {
        "id": "8rbdp45v3Bzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression with L2 Regularization (Ridge)\n",
        "model_l2 = LogisticRegression(penalty='l2', solver='lbfgs')  # 'lbfgs' supports L2 penalty\n",
        "model_l2.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_l2 = model_l2.predict(X_test)\n",
        "\n",
        "# Print model accuracy and coefficients\n",
        "print(\"L2 Regularization Model Accuracy:\", accuracy_score(y_test, y_pred_l2))\n",
        "print(\"Model Coefficients:\", model_l2.coef_)\n",
        "\n"
      ],
      "metadata": {
        "id": "f5yzQal13Ns0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression with Elastic Net Regularization\n",
        "model_en = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)  # 'saga' supports elastic net\n",
        "model_en.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_en = model_en.predict(X_test)\n",
        "\n",
        "# Print model accuracy\n",
        "print(\"Elastic Net Regularization Model Accuracy:\", accuracy_score(y_test, y_pred_en))\n",
        "\n"
      ],
      "metadata": {
        "id": "I0TRDR7H3USI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression for multiclass classification (One-vs-Rest)\n",
        "model_ovr = LogisticRegression(multi_class='ovr', solver='lbfgs')  \n",
        "model_ovr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_ovr = model_ovr.predict(X_test)\n",
        "\n",
        "# Print model accuracy\n",
        "print(\"OvR (One-vs-Rest) Multiclass Model Accuracy:\", accuracy_score(y_test, y_pred_ovr))\n",
        "\n"
      ],
      "metadata": {
        "id": "LmKp-HYK3XK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "\n"
      ],
      "metadata": {
        "id": "4DyM5GPf3bHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "model = LogisticRegression()\n",
        "\n",
        "cv_scores = cross_val_score(model, X, y, cv=skf)\n",
        "\n",
        "# Print average cross-validation accuracy\n",
        "print(\"Average CV Accuracy:\", np.mean(cv_scores))\n"
      ],
      "metadata": {
        "id": "fUHTnQZ53dFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from CSV file\n",
        "df = pd.read_csv(\"data.csv\")  # Replace with your actual CSV file\n",
        "\n",
        "# Assume the last column is the target variable\n",
        "X = df.iloc[:, :-1]  # Features\n",
        "y = df.iloc[:, -1]   # Target variable\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print model accuracy\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "n-m162kc3fQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_dist = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "}\n",
        "\n",
        "# Apply RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(LogisticRegression(), param_dist, cv=5, n_iter=10, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", random_search.best_score_)\n"
      ],
      "metadata": {
        "id": "nlaioMCC3mPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.\n",
        "\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply One-vs-One (OvO) Logistic Regression\n",
        "model_ovo = OneVsOneClassifier(LogisticRegression())\n",
        "model_ovo.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_ovo = model_ovo.predict(X_test)\n",
        "\n",
        "# Print model accuracy\n",
        "print(\"OvO Multiclass Model Accuracy:\", accuracy_score(y_test, y_pred_ovo))\n"
      ],
      "metadata": {
        "id": "PxtHasQo3o9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "j821fowq3rUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "5UYew_UW3tyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Generate an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with class weights\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "CIs_tkF53wnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset\n",
        "df = pd.read_csv(\"titanic.csv\")  # Replace with your actual dataset\n",
        "\n",
        "# Select features and target variable\n",
        "X = df[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
        "y = df['Survived']\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X = imputer.fit_transform(X)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "ivdGIr4L30wE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train without scaling\n",
        "model_no_scaling = LogisticRegression()\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "accuracy_no_scaling = accuracy_score(y_test, model_no_scaling.predict(X_test))\n",
        "\n",
        "# Apply standardization\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train with scaling\n",
        "model_scaled = LogisticRegression()\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "accuracy_scaled = accuracy_score(y_test, model_scaled.predict(X_test_scaled))\n",
        "\n",
        "print(f\"Accuracy without Scaling: {accuracy_no_scaling:.2f}\")\n",
        "print(f\"Accuracy with Scaling: {accuracy_scaled:.2f}\")\n"
      ],
      "metadata": {
        "id": "7PCvoXt731uC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "id": "XZV43tNI32qF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy with C=0.5:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "sbuF6-6i33rH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic dataset with feature names\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "feature_names = [f'Feature {i}' for i in range(X.shape[1])]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance (coefficients)\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
        "importance_df = importance_df.sort_values(by='Coefficient', ascending=False)\n",
        "\n",
        "print(\"Feature Importance in Logistic Regression:\")\n",
        "print(importance_df)\n"
      ],
      "metadata": {
        "id": "Qft0H--04HCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute Cohen's Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "print(\"Cohen's Kappa Score:\", kappa_score)\n"
      ],
      "metadata": {
        "id": "hJKjvh_B4Jco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute Precision-Recall curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PfTGFIGr4KWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=500)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Solver: {solver}, Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "eZvkvRuK4Rzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute MCC\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n"
      ],
      "metadata": {
        "id": "TuTN4HOb4TcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression on raw data\n",
        "model_raw = LogisticRegression()\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on standardized data\n",
        "model_scaled = LogisticRegression()\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy on Raw Data: {accuracy_raw:.4f}\")\n",
        "print(f\"Accuracy on Standardized Data: {accuracy_scaled:.4f}\")\n"
      ],
      "metadata": {
        "id": "NSvU3Tnh4UJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Define range of C values to test\n",
        "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "\n",
        "# Perform cross-validation for each C value\n",
        "best_C = None\n",
        "best_score = 0\n",
        "\n",
        "for C in C_values:\n",
        "    model = LogisticRegression(C=C)\n",
        "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
        "    avg_score = np.mean(scores)\n",
        "    print(f\"C={C}, Average Accuracy={avg_score:.4f}\")\n",
        "\n",
        "    if avg_score > best_score:\n",
        "        best_score = avg_score\n",
        "        best_C = C\n",
        "\n",
        "print(f\"Best C: {best_C} with Accuracy: {best_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "ySsS3p__4dcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.\n",
        "\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(model, 'logistic_regression_model.pkl')\n",
        "\n",
        "# Load the model\n",
        "loaded_model = joblib.load('logistic_regression_model.pkl')\n",
        "\n",
        "# Make predictions\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Loaded Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "Vey8aKaG4jTb"
      }
    }
  ]
}