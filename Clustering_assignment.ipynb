{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0TOQMe8MM32"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theory Questions:\n",
        "1. What is unsupervised learning in the context of machine learning?\n",
        "\n",
        "Unsupervised learning is a type of machine learning where the model is trained on data without labeled outputs. The goal is to uncover hidden patterns or groupings in the data, such as clustering or dimensionality reduction.\n",
        "\n",
        "2. How does K-Means clustering algorithm work?\n",
        "\n",
        "K-Means partitions data into k clusters by initializing k centroids, assigning each point to the nearest centroid, then updating centroids as the mean of assigned points. This repeats until centroids stabilize.\n",
        "\n",
        "3. Explain the concept of a dendrogram in hierarchical clustering.\n",
        "\n",
        "A dendrogram is a tree-like diagram that shows the arrangement of the clusters formed at each step of hierarchical clustering. It helps visualize how clusters are merged or split at various levels.\n",
        "\n",
        "4. What is the main difference between K-Means and Hierarchical Clustering?\n",
        "\n",
        "K-Means requires a predefined number of clusters and is a partitional method. Hierarchical clustering does not need a specified number of clusters and builds a tree of clusters.\n",
        "\n",
        "5. What are the advantages of DBSCAN over K-Means?\n",
        "\n",
        "No need to specify the number of clusters.\n",
        "Can find clusters of arbitrary shapes.\n",
        "Handles noise/outliers well.\n",
        "\n",
        "6. When would you use Silhouette Score in clustering?\n",
        "\n",
        "Silhouette Score is used to evaluate the quality of clustering. It measures how similar a point is to its own cluster compared to other clusters and helps determine the optimal number of clusters.\n",
        "\n",
        "7. What are the limitations of Hierarchical Clustering?\n",
        "\n",
        "Computationally expensive for large datasets.\n",
        "Once a merge/split is done, it cannot be undone.\n",
        "Sensitive to noise and outliers.\n",
        "Does not scale well.\n",
        "\n",
        "8. Why is feature scaling important in clustering algorithms like K-Means?\n",
        "\n",
        "K-Means relies on distance calculations (e.g., Euclidean distance). If features are on different scales, one feature may dominate the clustering, leading to poor results.\n",
        "\n",
        "9. How does DBSCAN identify noise points?\n",
        "\n",
        "Points that are not core points and do not fall within the neighborhood (ε) of any core point are classified as noise or outliers.\n",
        "\n",
        "10. Define inertia in the context of K-Means.\n",
        "\n",
        "Inertia is the sum of squared distances between each point and the centroid of its cluster. Lower inertia indicates tighter, more compact clusters.\n",
        "\n",
        "11. What is the elbow method in K-Means clustering?\n",
        "\n",
        "A method to determine the optimal number of clusters by plotting inertia vs. number of clusters. The \"elbow\" point where inertia starts to decrease slowly is chosen as the ideal number of clusters.\n",
        "\n",
        "12. Describe the concept of \"density\" in DBSCAN.\n",
        "\n",
        "Density refers to the number of points within a specified radius (ε). A region is considered dense if it has at least a minimum number of points (MinPts) within ε.\n",
        "\n",
        "13. Can hierarchical clustering be used on categorical data?\n",
        "\n",
        "Yes, but standard hierarchical clustering using Euclidean distance is not ideal. Special distance measures (e.g., Hamming distance) and techniques (e.g., using Gower’s metric) are needed.\n",
        "\n",
        "14. What does a negative Silhouette Score indicate?\n",
        "\n",
        "A negative Silhouette Score means a point is closer to a neighboring cluster than to its own, indicating it might be misclassified.\n",
        "\n",
        "15. Explain the term \"linkage criteria\" in hierarchical clustering.\n",
        "\n",
        "Linkage criteria determine how the distance between clusters is calculated. Common types include:\n",
        "Single linkage (min distance),\n",
        "Complete linkage (max distance),\n",
        "Average linkage (mean distance),\n",
        "Ward’s method (minimize variance).\n",
        "\n",
        "16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?\n",
        "\n",
        "Because K-Means assumes clusters of similar size and density. It uses centroid-based assignment and may misclassify points in clusters with different shapes, sizes, or densities.\n",
        "\n",
        "17. What are the core parameters in DBSCAN, and how do they influence clustering?\n",
        "\n",
        "ε (epsilon): Radius to consider for neighborhood.\n",
        "MinPts: Minimum number of points within ε to form a dense region.\n",
        "These parameters determine what is considered a cluster and what is labeled noise.\n",
        "\n",
        "18. How does K-Means++ improve upon standard K-Means initialization?\n",
        "\n",
        "K-Means++ selects initial centroids in a smarter way, maximizing the distance between them. This reduces chances of poor clustering and speeds up convergence.\n",
        "\n",
        "19. What is agglomerative clustering?\n",
        "\n",
        "A type of hierarchical clustering that starts with each point as its own cluster and iteratively merges the closest pairs until one cluster or desired number is left.\n",
        "\n",
        "20. What makes Silhouette Score a better metric than just inertia for model evaluation?\n",
        "\n",
        "Inertia only measures intra-cluster distance, not inter-cluster separation. Silhouette Score considers both compactness and separation, giving a more balanced view of clustering quality."
      ],
      "metadata": {
        "id": "eew25AXSMS47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRACTICAL QUESTIONS"
      ],
      "metadata": {
        "id": "MB2xyc_KMkuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot.\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_blobs(n_samples=500, centers=4, cluster_std=0.6, random_state=42)\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "y_pred = kmeans.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=30)\n",
        "plt.title(\"K-Means Clustering on Synthetic Data (4 centers)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yqnt4-3SMlKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "agg = AgglomerativeClustering(n_clusters=3)\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "print(\"First 10 predicted labels:\", labels[:10])\n"
      ],
      "metadata": {
        "id": "B4PlMo7oN9sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot.\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='plasma', s=30)\n",
        "outliers = labels == -1\n",
        "plt.scatter(X[outliers, 0], X[outliers, 1], c='red', s=50, label='Outliers')\n",
        "plt.legend()\n",
        "plt.title(\"DBSCAN on make_moons data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f7dylBkTODc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "cluster_sizes = Counter(labels)\n",
        "print(\"Cluster sizes:\", cluster_sizes)\n"
      ],
      "metadata": {
        "id": "hPBik972OFCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Use make circles to generate synthetic data and cluster it using DBSCAN. Plot the result.\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "X, _ = make_circles(n_samples=300, noise=0.05, factor=0.4, random_state=42)\n",
        "dbscan = DBSCAN(eps=0.15, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='coolwarm', s=30)\n",
        "plt.title(\"DBSCAN clustering on make_circles\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "j-dhgJF3OGTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "centroids = pd.DataFrame(kmeans.cluster_centers_, columns=cancer.feature_names)\n",
        "print(\"Cluster centroids:\\n\", centroids)\n"
      ],
      "metadata": {
        "id": "iO8Mz5GROIad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27. Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN.\n",
        "X, _ = make_blobs(n_samples=500, centers=3, cluster_std=[0.3, 1.5, 0.5], random_state=42)\n",
        "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Set1', s=30)\n",
        "plt.title(\"DBSCAN on blobs with varying std dev\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "n0llKVryOJqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28. Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means.\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab10', s=30)\n",
        "plt.title(\"K-Means clusters on Digits PCA-reduced data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DMaeeGoZOK8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29. Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "X, _ = make_blobs(n_samples=500, centers=4, cluster_std=0.7, random_state=42)\n",
        "scores = []\n",
        "\n",
        "for k in range(2, 6):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    score = silhouette_score(X, labels)\n",
        "    scores.append(score)\n",
        "\n",
        "plt.bar(range(2, 6), scores, color='skyblue')\n",
        "plt.xlabel(\"Number of clusters (k)\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.title(\"Silhouette Scores for different k values\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yHrZN_3tOMaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30. Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage.\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "linked = linkage(X, method='average')\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "dendrogram(linked, labels=iris.target, orientation='top')\n",
        "plt.title(\"Dendrogram (Average linkage) for Iris dataset\")\n",
        "plt.xlabel(\"Samples\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lIpmiECsOOJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#31. Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with decision boundaries.\n",
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=3, cluster_std=2.5, random_state=42)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Create mesh grid for decision boundaries\n",
        "h = 0.1\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis', edgecolor='k')\n",
        "plt.title(\"K-Means with overlapping clusters and decision boundaries\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-y1kkrZCOPMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#32. Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results.\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "dbscan = DBSCAN(eps=3, min_samples=5)\n",
        "labels = dbscan.fit_predict(X_tsne)\n",
        "\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab20', s=30)\n",
        "plt.title(\"DBSCAN on t-SNE reduced Digits data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7GPIl78ZOQOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#33. Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot the result.\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n",
        "agg = AgglomerativeClustering(n_clusters=3, linkage='complete')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Set1', s=30)\n",
        "plt.title(\"Agglomerative Clustering with complete linkage\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8Qd5apOfORmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#34. Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means, Show results in a line plot.\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "\n",
        "inertia_values = []\n",
        "k_range = range(2, 7)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X)\n",
        "    inertia_values.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(k_range, inertia_values, marker='o')\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.title(\"K-Means Inertia for Breast Cancer dataset\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eKBKTo6TOSyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#35. Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage.\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)\n",
        "agg = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.title(\"Agglomerative Clustering (Single linkage) on Concentric Circles\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "F04puH8FOUe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#36. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.datasets import load_wine\n",
        "import numpy as np\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "dbscan = DBSCAN(eps=1.5, min_samples=5)\n",
        "labels = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "print(\"Number of clusters (excluding noise):\", n_clusters)\n"
      ],
      "metadata": {
        "id": "Sgj-IipeOZrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#37. Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the data points.\n",
        "X, _ = make_blobs(n_samples=400, centers=4, cluster_std=0.7, random_state=42)\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=30)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            s=200, c='red', marker='X')\n",
        "plt.title(\"K-Means with cluster centers\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5cZI1yRLObJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#38. Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise.\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "noise_count = list(labels).count(-1)\n",
        "print(\"Number of noise samples detected:\", noise_count)\n"
      ],
      "metadata": {
        "id": "5q02R7XoOdVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#39. Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result.\n",
        "X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='coolwarm')\n",
        "plt.title(\"K-Means clustering on make_moons (non-linear data)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3A7wGtxGOeZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#40. Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D scatter plot.\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=labels, cmap='tab10', s=40)\n",
        "ax.set_title(\"3D PCA + K-Means on Digits dataset\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iqV2FL4-OgDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#41. Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the clustering.\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "X, _ = make_blobs(n_samples=500, centers=5, cluster_std=0.7, random_state=42)\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "score = silhouette_score(X, labels)\n",
        "print(\"Silhouette Score for 5-cluster KMeans:\", score)\n"
      ],
      "metadata": {
        "id": "x7knk9W3OhX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#42. Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 2D.\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=2)\n",
        "labels = agg.fit_predict(X_pca)\n",
        "\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', s=30)\n",
        "plt.title(\"Agglomerative Clustering after PCA on Breast Cancer data\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Os_kh_mIOm6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#43. Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN side-by-side.\n",
        "X, _ = make_circles(n_samples=300, noise=0.1, factor=0.4, random_state=42)\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "labels_km = kmeans.fit_predict(X)\n",
        "\n",
        "dbscan = DBSCAN(eps=0.15, min_samples=5)\n",
        "labels_db = dbscan.fit_predict(X)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12,5))\n",
        "\n",
        "axs[0].scatter(X[:, 0], X[:, 1], c=labels_km, cmap='coolwarm')\n",
        "axs[0].set_title(\"KMeans Clustering\")\n",
        "\n",
        "axs[1].scatter(X[:, 0], X[:, 1], c=labels_db, cmap='coolwarm')\n",
        "axs[1].set_title(\"DBSCAN Clustering\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YJ9YrgBNOoCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#44. Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering.\n",
        "from sklearn.metrics import silhouette_samples\n",
        "import seaborn as sns\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "sil_samples = silhouette_samples(X, labels)\n",
        "\n",
        "sns.scatterplot(x=range(len(sil_samples)), y=sil_samples, hue=labels, palette='Set1', legend='full')\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Silhouette Coefficient\")\n",
        "plt.title(\"Silhouette Coefficient per sample (Iris + KMeans)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Xds6dlhnOqb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#45. Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage. Visualize clusters.\n",
        "X, _ = make_blobs(n_samples=400, centers=4, cluster_std=0.8, random_state=42)\n",
        "agg = AgglomerativeClustering(n_clusters=4, linkage='average')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Set2', s=30)\n",
        "plt.title(\"Agglomerative Clustering (Average linkage)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kRRKknilOryt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#46. Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4 features).\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data[:, :4]\n",
        "df = pd.DataFrame(X, columns=wine.feature_names[:4])\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df['cluster'] = kmeans.fit_predict(X).astype(str)\n",
        "\n",
        "sns.pairplot(df, hue='cluster', palette='Set1')\n",
        "plt.suptitle(\"Wine dataset cluster assignments (first 4 features)\", y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FRBX0CC3Otk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#47. Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the count.\n",
        "X, _ = make_blobs(n_samples=500, centers=3, cluster_std=1.5, random_state=42)\n",
        "dbscan = DBSCAN(eps=1.2, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise = list(labels).count(-1)\n",
        "\n",
        "print(f\"Clusters found: {n_clusters}\")\n",
        "print(f\"Noise points detected: {n_noise}\")\n"
      ],
      "metadata": {
        "id": "dknGDfXCOupV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#48. Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters\n",
        "X = load_digits().data\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=10)\n",
        "labels = agg.fit_predict(X_tsne)\n",
        "\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', s=30)\n",
        "plt.title(\"Agglomerative Clustering on t-SNE reduced Digits data\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "9FVhg5yKOwMo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}