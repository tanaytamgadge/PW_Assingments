{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Regression Assignment"
      ],
      "metadata": {
        "id": "Z0Njfo-kc1Fm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theory Questions\n",
        "Q1: What does R-squared represent in a regression model?\n",
        "\n",
        "A: R-squared represents the proportion of variance in the dependent variable that's explained by the independent variables. It ranges from 0 to 1, where 1 means the model explains all variability in the data.\n",
        "\n",
        "Q2: What is the difference between R-squared and Adjusted R-squared?\n",
        "\n",
        "A: - R-squared increases or stays the same when adding any variable\n",
        "- Adjusted R-squared penalizes for adding variables that don't improve the model\n",
        "- Adjusted R-squared can decrease when adding irrelevant variables\n",
        "\n",
        "Q3: What does an Adjusted R-squared value of 0.85 indicate?\n",
        "\n",
        "A: An Adjusted R-squared of 0.85 indicates that 85% of the variance in the dependent variable is explained by the predictors, accounting for the number of predictors in the model.\n",
        "\n",
        "Q4: What does a high R-squared value mean?\n",
        "\n",
        "A: A high R-squared indicates:\n",
        "- Strong fit between predictors and target\n",
        "- Large proportion of variance explained\n",
        "- Model captures patterns well\n",
        "- May suggest overfitting if extremely high\n",
        "\n",
        "Q5: How is Adjusted R-squared calculated?\n",
        "\n",
        "A: Adjusted R-squared calculation:\n",
        "1 - [(1 - R²)(n-1)/(n-k-1)]\n",
        "where:\n",
        "n = sample size\n",
        "k = number of predictors\n",
        "\n",
        "ERROR METRICS\n",
        "\n",
        "Q6: Why do we use Mean Squared Error (MSE)?\n",
        "\n",
        "A: MSE is used because it:\n",
        "- Penalizes larger errors more heavily (squared term)\n",
        "- Provides a single metric for model evaluation\n",
        "- Is differentiable (useful for optimization)\n",
        "- Always yields positive values\n",
        "\n",
        "Q7: What is Mean Absolute Error (MAE)?\n",
        "\n",
        "A: MAE is the average absolute difference between predicted and actual values. It's:\n",
        "- More robust to outliers than MSE\n",
        "- Easier to interpret\n",
        "- Represents average error in original units\n",
        "\n",
        "Q8: Why is RMSE considered more interpretable than MSE?\n",
        "\n",
        "A: RMSE is more interpretable than MSE because:\n",
        "- It's in the same units as the target variable\n",
        "- Provides a more intuitive error magnitude\n",
        "- Easier to compare with the original scale\n",
        "\n",
        "Q9: Why is MSE sensitive to outliers?\n",
        "\n",
        "A: MSE is sensitive to outliers because:\n",
        "- Errors are squared\n",
        "- Large deviations are heavily penalized\n",
        "- Outliers have disproportionate impact\n",
        "- Can distort model evaluation\n",
        "\n",
        "Q10: What is Root Mean Squared Error (RMSE)?\n",
        "\n",
        "A: RMSE is:\n",
        "- Square root of MSE\n",
        "- Average deviation in original units\n",
        "- Common metric for model evaluation\n",
        "- More sensitive to outliers than MAE\n",
        "\n",
        "LINEAR REGRESSION ASSUMPTIONS & ISSUES\n",
        "\n",
        "Q11: What are the assumptions of linear regression?\n",
        "\n",
        "A: Linear regression assumptions include:\n",
        "- Linearity: Linear relationship between X and Y\n",
        "- Independence of errors\n",
        "- Homoscedasticity: Constant variance of residuals\n",
        "- Normality of residuals\n",
        "- No perfect multicollinearity\n",
        "- Independent observations\n",
        "\n",
        "Q12: How do we check for normality of residuals in linear regression?\n",
        "\n",
        "A: Normality of residuals can be checked through:\n",
        "- Q-Q plots\n",
        "- Histogram of residuals\n",
        "- Shapiro-Wilk test\n",
        "- Anderson-Darling test\n",
        "- Visual inspection of residual plots\n",
        "\n",
        "Q13: What is multicollinearity, and how does it impact regression?\n",
        "\n",
        "A: Multicollinearity occurs when independent variables are highly correlated. It:\n",
        "- Makes coefficient estimates unstable\n",
        "- Increases standard errors\n",
        "- Makes it difficult to determine individual variable importance\n",
        "- Doesn't affect overall model predictions\n",
        "\n",
        "Q14: What happens if linear regression assumptions are violated?\n",
        "\n",
        "A: Violation of assumptions can lead to:\n",
        "- Biased coefficient estimates\n",
        "- Incorrect standard errors\n",
        "- Invalid hypothesis tests\n",
        "- Unreliable predictions\n",
        "- Misleading R-squared values\n",
        "\n",
        "Q15: What is the role of homoscedasticity in linear regression?\n",
        "\n",
        "A: Homoscedasticity ensures:\n",
        "- Constant variance of residuals\n",
        "- Valid standard errors\n",
        "- Reliable hypothesis tests\n",
        "- Efficient parameter estimates\n",
        "\n",
        "HANDLING ISSUES & SOLUTIONS\n",
        "\n",
        "Q16: How can we address multicollinearity in regression?\n",
        "\n",
        "A: Addressing multicollinearity:\n",
        "- Remove highly correlated variables\n",
        "- Use principal component analysis (PCA)\n",
        "- Ridge regression\n",
        "- Create composite variables\n",
        "- Collect more data\n",
        "\n",
        "Q17: What is heteroscedasticity, and why is it a problem?\n",
        "\n",
        "A: Heteroscedasticity:\n",
        "- Non-constant variance of residuals\n",
        "- Leads to inefficient estimates\n",
        "- Invalidates standard errors\n",
        "- Makes prediction intervals unreliable\n",
        "\n",
        "Q18: How does adding irrelevant predictors affect R-squared and Adjusted R-squared?\n",
        "\n",
        "A: Effect on R² and Adjusted R²:\n",
        "- R² increases with irrelevant predictors\n",
        "- Adjusted R² decreases with irrelevant predictors\n",
        "- Shows why Adjusted R² is preferred\n",
        "- Helps prevent overfitting\n",
        "\n",
        "ML PIPELINES & MODEL SAVING\n",
        "\n",
        "Q19: What are the benefits of using an ML pipeline?\n",
        "\n",
        "A: ML pipeline benefits:\n",
        "- Ensures consistent preprocessing\n",
        "- Reduces code duplication\n",
        "- Prevents data leakage\n",
        "- Makes deployment easier\n",
        "- Improves reproducibility\n",
        "\n",
        "Q20: Why do we use pipelines in machine learning?\n",
        "\n",
        "A: Pipelines are used because they:\n",
        "- Ensure proper order of operations\n",
        "- Prevent data leakage\n",
        "- Simplify model deployment\n",
        "- Make cross-validation easier\n",
        "- Enable proper scaling/transformation\n",
        "\n",
        "Q21: What is pickling in Python, and how is it useful in ML?\n",
        "\n",
        "A: Pickling in Python:\n",
        "- Serializes Python objects to binary format\n",
        "- Allows saving trained models to disk\n",
        "- Enables model sharing and deployment\n",
        "- Preserves the entire object state\n",
        "\n",
        "Q22: Why is pickling considered risky?\n",
        "\n",
        "A: Pickling risks include:\n",
        "- Security vulnerabilities\n",
        "- Version compatibility issues\n",
        "- Platform dependencies\n",
        "- Potential code execution risks\n",
        "- Size inefficiency\n",
        "\n",
        "Q23: What alternatives exist to pickling for saving ML models?\n",
        "\n",
        "A: Alternatives to pickling:\n",
        "- joblib\n",
        "- ONNX format\n",
        "- TensorFlow SavedModel\n",
        "- PMML\n",
        "- Custom serialization methods"
      ],
      "metadata": {
        "id": "oA6xYWqzfEW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRACTICAL QUESTIONS"
      ],
      "metadata": {
        "id": "NzBBN2RVe79c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 1 ANSWER\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Load dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Preprocessing\n",
        "X = pd.get_dummies(diamonds.drop('price', axis=1), drop_first=True)\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Errors\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "print(f\"MSE: {mse:.2f}, MAE: {mae:.2f}\")\n"
      ],
      "metadata": {
        "id": "RNXYR3G9c2hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 2 ANSWER\n",
        "from sklearn.datasets import make_regression\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Errors\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"MSE: {mse:.2f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n"
      ],
      "metadata": {
        "id": "JtWNmRwFdG-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 3 ANSWER\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Linearity\n",
        "sns.scatterplot(x=y_test, y=y_pred)\n",
        "plt.title('Linearity Check')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "# Residuals plot for homoscedasticity\n",
        "residuals = y_test - y_pred\n",
        "sns.scatterplot(x=y_pred, y=residuals)\n",
        "plt.axhline(0, color='r', linestyle='--')\n",
        "plt.title('Residuals Plot (Homoscedasticity)')\n",
        "plt.show()\n",
        "\n",
        "# Correlation matrix for multicollinearity\n",
        "sns.heatmap(pd.DataFrame(X_train).corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5IjFgetEdOBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 4 ANSWER\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "score = pipeline.score(X_test, y_test)\n",
        "\n",
        "print(f\"R-squared score: {score:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zS8WNms-dSzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5.\n",
        "# Model fitting\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Coefficients, Intercept, and R-squared\n",
        "print(f\"Coefficients: {model.coef_}\")\n",
        "print(f\"Intercept: {model.intercept_}\")\n",
        "print(f\"R-squared score: {model.score(X_test, y_test):.2f}\")\n"
      ],
      "metadata": {
        "id": "hBv5mD1cdXZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. # Load dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Prepare data\n",
        "X = tips[['total_bill']]\n",
        "y = tips['tip']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Slope and Intercept\n",
        "print(f\"Slope: {model.coef_[0]:.2f}\")\n",
        "print(f\"Intercept: {model.intercept_:.2f}\")\n"
      ],
      "metadata": {
        "id": "I-jnEv7sdcrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. import numpy as np\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2 * X + np.random.randn(100, 1) * 2\n",
        "\n",
        "# Model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, label='Data points')\n",
        "plt.plot(X, y_pred, color='r', label='Regression line')\n",
        "plt.legend()\n",
        "plt.title('Simple Linear Regression')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TRXaFgnndf82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8. import pickle\n",
        "\n",
        "# Save the model\n",
        "with open('linear_model.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "# Load the model\n",
        "with open('linear_model.pkl', 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "\n",
        "print(f\"Loaded Model Coefficients: {loaded_model.coef_}\")\n"
      ],
      "metadata": {
        "id": "Lh-wiXNaditI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9.from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Polynomial Features\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Model\n",
        "poly_model = LinearRegression()\n",
        "poly_model.fit(X_poly, y)\n",
        "\n",
        "# Predictions\n",
        "y_poly_pred = poly_model.predict(X_poly)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, label='Data points')\n",
        "plt.plot(X, y_poly_pred, color='r', label='Polynomial regression curve')\n",
        "plt.legend()\n",
        "plt.title('Polynomial Regression (Degree 2)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aGkE0UUIdjw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10.# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 3 * X + np.random.randn(100, 1) * 2\n",
        "\n",
        "# Model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Coefficient and Intercept\n",
        "print(f\"Coefficient: {model.coef_[0][0]:.2f}\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.2f}\")\n"
      ],
      "metadata": {
        "id": "ctDQkc2UdlT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11.# Polynomial Features\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Model\n",
        "poly_model = LinearRegression()\n",
        "poly_model.fit(X_poly, y)\n",
        "\n",
        "# Predictions\n",
        "y_poly_pred = poly_model.predict(X_poly)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, label='Data points')\n",
        "plt.plot(X, y_poly_pred, color='r', label='Polynomial regression curve')\n",
        "plt.legend()\n",
        "plt.title('Polynomial Regression (Degree 3)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O_QKNG9sdm2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "12.# Generate synthetic data\n",
        "X, y = make_regression(n_samples=100, n_features=2, noise=10, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Coefficients, Intercept, and R-squared\n",
        "print(f\"Coefficients: {model.coef_}\")\n",
        "print(f\"Intercept: {model.intercept_}\")\n",
        "print(f\"R-squared score: {model.score(X_test, y_test):.2f}\")\n"
      ],
      "metadata": {
        "id": "5VhSUj2LdpnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "13.# Generate synthetic data\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=20, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Errors\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"MSE: {mse:.2f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n"
      ],
      "metadata": {
        "id": "8BeHDjVLdqWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "14.from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import pandas as pd\n",
        "\n",
        "# Example synthetic dataset with multiple features\n",
        "X, _ = make_regression(n_samples=100, n_features=3, noise=10, random_state=42)\n",
        "X_df = pd.DataFrame(X, columns=['Feature1', 'Feature2', 'Feature3'])\n",
        "\n",
        "# Calculate VIF\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X_df.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_df.values, i) for i in range(X_df.shape[1])]\n",
        "\n",
        "print(vif_data)\n"
      ],
      "metadata": {
        "id": "CRkqfzLCdrfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "15.# Polynomial Features\n",
        "poly = PolynomialFeatures(degree=4)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Model\n",
        "poly_model = LinearRegression()\n",
        "poly_model.fit(X_poly, y)\n",
        "\n",
        "# Predictions\n",
        "y_poly_pred = poly_model.predict(X_poly)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, label='Data points')\n",
        "plt.plot(X, y_poly_pred, color='r', label='Polynomial regression curve')\n",
        "plt.legend()\n",
        "plt.title('Polynomial Regression (Degree 4)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Aep8VIYWdsUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "16.# Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "score = pipeline.score(X_test, y_test)\n",
        "\n",
        "print(f\"R-squared score: {score:.2f}\")\n"
      ],
      "metadata": {
        "id": "e2NYyagVdtaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "17.# Polynomial Features\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Model\n",
        "poly_model = LinearRegression()\n",
        "poly_model.fit(X_poly, y)\n",
        "\n",
        "# Predictions\n",
        "y_poly_pred = poly_model.predict(X_poly)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, label='Data points')\n",
        "plt.plot(X, y_poly_pred, color='r', label='Polynomial regression curve')\n",
        "plt.legend()\n",
        "plt.title('Polynomial Regression (Degree 3)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bq5pAJ5yduQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "18.# Generate synthetic data\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(f\"R-squared score: {model.score(X_test, y_test):.2f}\")\n",
        "print(f\"Coefficients: {model.coef_}\")\n"
      ],
      "metadata": {
        "id": "99U97f3pdvXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "19.# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 4 * X + np.random.randn(100, 1) * 3\n",
        "\n",
        "# Model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, label='Data points')\n",
        "plt.plot(X, y_pred, color='r', label='Regression line')\n",
        "plt.legend()\n",
        "plt.title('Linear Regression Visualization')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7jonePcidwZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "20.# Generate synthetic data\n",
        "X, y = make_regression(n_samples=100, n_features=3, noise=10, random_state=42)\n",
        "\n",
        "# Model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Results\n",
        "print(f\"R-squared score: {model.score(X, y):.2f}\")\n",
        "print(f\"Coefficients: {model.coef_}\")\n"
      ],
      "metadata": {
        "id": "EibQtspxdxd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "21.# Save the model\n",
        "with open('linear_model.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "# Load the model\n",
        "with open('linear_model.pkl', 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "\n",
        "# Prediction\n",
        "new_data = [[2.5, 3.5, 1.2]]\n",
        "prediction = loaded_model.predict(new_data)\n",
        "print(f\"Prediction: {prediction}\")\n"
      ],
      "metadata": {
        "id": "gBViFHdYdyyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "22.from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# One-hot encoding\n",
        "column_transformer = ColumnTransformer([\n",
        "    ('encoder', OneHotEncoder(), ['sex', 'smoker', 'day', 'time'])\n",
        "], remainder='passthrough')\n",
        "\n",
        "X = column_transformer.fit_transform(tips.drop('tip', axis=1))\n",
        "y = tips['tip']\n",
        "\n",
        "# Model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(f\"R-squared score: {model.score(X, y):.2f}\")\n"
      ],
      "metadata": {
        "id": "L3UMrerId0Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "23.from sklearn.linear_model import Ridge\n",
        "\n",
        "# Ridge Regression\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(\"Linear Regression:\")\n",
        "print(f\"Coefficients: {model.coef_}\")\n",
        "print(f\"R-squared: {model.score(X_test, y_test):.2f}\")\n",
        "\n",
        "print(\"Ridge Regression:\")\n",
        "print(f\"Coefficients: {ridge.coef_}\")\n",
        "print(f\"R-squared: {ridge.score(X_test, y_test):.2f}\")\n"
      ],
      "metadata": {
        "id": "DTKFw74Hd0qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "24.from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "\n",
        "print(f\"Cross-validated R-squared scores: {scores}\")\n",
        "print(f\"Mean R-squared score: {scores.mean():.2f}\")\n"
      ],
      "metadata": {
        "id": "nDPfFgTHd1NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "25.from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "\n",
        "print(f\"Cross-validated R-squared scores: {scores}\")\n",
        "print(f\"Mean R-squared score: {scores.mean():.2f}\")\n"
      ],
      "metadata": {
        "id": "jXK41RGhd2fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "26."
      ],
      "metadata": {
        "id": "XMhmv4y4d3KT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "58ltMZA5d3pp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}