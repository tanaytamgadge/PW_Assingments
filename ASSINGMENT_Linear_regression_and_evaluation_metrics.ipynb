{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxTSA_2vvMIT"
      },
      "outputs": [],
      "source": [
        "THEORY QUESTIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What does R-squared represent in a regression model?\n",
        "R-squared, or the coefficient of determination, represents the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, where 1 indicates that the model explains all the variability of the response data around its mean.\n",
        "\n",
        "2. What are the assumptions of linear regression?\n",
        "The key assumptions are:\n",
        "Linearity: The relationship between the independent and dependent variables is linear.\n",
        "Independence: The residuals (errors) are independent.\n",
        "Homoscedasticity: The residuals have constant variance at every level of the independent variable.\n",
        "Normality: The residuals are normally distributed.\n",
        "No multicollinearity: The independent variables are not highly correlated.\n",
        "\n",
        "3. What is the difference between R-squared and Adjusted R-squared?\n",
        "Adjusted R-squared adjusts the R-squared value for the number of predictors in the model. It penalizes the addition of irrelevant predictors and provides a more accurate measure of model performance.\n",
        "\n",
        "4. Why do we use Mean Squared Error (MSE)?\n",
        "MSE is used to measure the average of the squares of the errors. It indicates how well the regression line fits the data points. Lower MSE values indicate better fit.\n",
        "\n",
        "5. What does an Adjusted R-squared value of 0.85 indicate?\n",
        "An Adjusted R-squared value of 0.85 indicates that 85% of the variance in the dependent variable is explained by the model, adjusted for the number of predictors.\n",
        "\n",
        "6. How do we check for normality of residuals in linear regression?\n",
        "Normality of residuals can be checked using visual tools like histograms, Q-Q plots, and statistical tests like the Shapiro-Wilk test.\n",
        "\n",
        "7. What is multicollinearity, and how does it impact regression?\n",
        "Multicollinearity occurs when independent variables are highly correlated. It can inflate the variance of the coefficient estimates and make the model unstable.\n",
        "\n",
        "8. What is Mean Absolute Error (MAE)?\n",
        "MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It’s the average over the test sample of the absolute differences between prediction and actual observation.\n",
        "\n",
        "9. What are the benefits of using an ML pipeline?\n",
        "ML pipelines automate the workflow for machine learning models, ensuring consistent and efficient processing of data, transformation, and modeling.\n",
        "\n",
        "10. Why is RMSE considered more interpretable than MSE?\n",
        "RMSE is in the same units as the dependent variable, making it easier to interpret compared to MSE, which is in squared units.\n",
        "\n",
        "11. What is pickling in Python, and how is it useful in ML?\n",
        "Pickling is a way to serialize and deserialize Python objects. It’s useful for saving ML models and other data structures to disk.\n",
        "\n",
        "12. What does a high R-squared value mean?\n",
        "A high R-squared value indicates that the model explains a large portion of the variance in the dependent variable.\n",
        "\n",
        "13. What happens if linear regression assumptions are violated?\n",
        "Violating assumptions can lead to biased estimates, invalid statistical tests, and unreliable predictions.\n",
        "\n",
        "14. How can we address multicollinearity in regression?\n",
        "Multicollinearity can be addressed by removing or combining correlated predictors, using principal component analysis, or ridge regression.\n",
        "\n",
        "15. Why do we use pipelines in machine learning?\n",
        "Pipelines streamline the process of data transformation and model training, ensuring reproducibility and reducing the chances of data leakage.\n",
        "\n",
        "16. How is Adjusted R-squared calculated?\n",
        "Formula:\n",
        "Adjusted R-squared = 1 - [(1 - R^2) * (n - 1) / (n - k - 1)]\n",
        "\n",
        "where:\n",
        "R^2 is the R-squared value\n",
        "n is the number of data points\n",
        "k is the number of independent variables\n",
        "\n",
        "\n",
        "17. Why is MSE sensitive to outliers?\n",
        "MSE squares the errors, giving more weight to larger errors, thus making it sensitive to outliers.\n",
        "\n",
        "18. What is the role of homoscedasticity in linear regression?\n",
        "Homoscedasticity ensures that the variance of errors is constant across all levels of the independent variables, which is essential for valid statistical inference.\n",
        "\n",
        "19. What is Root Mean Squared Error (RMSE)?\n",
        "RMSE is the square root of the average of squared differences between predicted and observed values, providing a measure of how well the model fits the data.\n",
        "\n",
        "20. Why is pickling considered risky?\n",
        "Pickling can execute arbitrary code if the pickle file is tampered with, posing a security risk.\n",
        "\n",
        "21. What alternatives exist to pickling for saving ML models?\n",
        "Alternatives include using libraries like Joblib, or formats like ONNX and PMML.\n",
        "\n",
        "22. What is heteroscedasticity, and why is it a problem?\n",
        "Heteroscedasticity occurs when the variance of errors differs across levels of the independent variables, leading to inefficient and biased estimates.\n",
        "\n",
        "23. How does adding irrelevant predictors affect R-squared and Adjusted R-squared?\n",
        "Adding irrelevant predictors can increase R-squared but decrease Adjusted R-squared, as the latter penalizes unnecessary complexity.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ae-6ZIcCvZxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PRACTICAL QUESTIONS\n",
        "\n",
        "1.\n",
        "# Import necessary libraries\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Load the diamonds dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Drop rows with missing values\n",
        "diamonds = diamonds.dropna()\n",
        "\n",
        "# Select features and target variable\n",
        "X = diamonds[['carat', 'depth', 'table']]  # Example features\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")"
      ],
      "metadata": {
        "id": "nyRmfNRmyJcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2.\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Load the diamonds dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Drop rows with missing values\n",
        "diamonds = diamonds.dropna()\n",
        "\n",
        "# Select features and target variable\n",
        "X = diamonds[['carat', 'depth', 'table']]  # Example features\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Calculate Root Mean Squared Error (RMSE)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
      ],
      "metadata": {
        "id": "YxiO7xHfySrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3.\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "# Load the diamonds dataset\n",
        "diamonds = sns.load_dataset('diamonds').dropna()\n",
        "\n",
        "# Select features and target variable\n",
        "X = diamonds[['carat', 'depth', 'table']]\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Check linearity\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.title('Linearity Check')\n",
        "plt.xlabel('Actual Prices')\n",
        "plt.ylabel('Predicted Prices')\n",
        "plt.show()\n",
        "\n",
        "# Check homoscedasticity\n",
        "residuals = y_test - y_pred\n",
        "plt.scatter(y_pred, residuals)\n",
        "plt.title('Homoscedasticity Check')\n",
        "plt.xlabel('Predicted Prices')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()\n",
        "\n",
        "# Check multicollinearity\n",
        "X_vif = add_constant(X)\n",
        "vif = pd.DataFrame()\n",
        "vif[\"VIF Factor\"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
        "vif[\"features\"] = X_vif.columns\n",
        "print(vif)\n"
      ],
      "metadata": {
        "id": "9p_t77O_ydTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4.\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_pipeline = pipeline.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred_pipeline)\n",
        "print(f\"Pipeline R-squared Score: {r2}\")\n"
      ],
      "metadata": {
        "id": "6NBFCEa1ykrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5.\n",
        "# Simple Linear Regression Example using tips dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Features and target variable\n",
        "X_tips = tips[['total_bill']]\n",
        "y_tips = tips['tip']\n",
        "\n",
        "# Split the data\n",
        "X_train_tips, X_test_tips, y_train_tips, y_test_tips = train_test_split(X_tips, y_tips, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the linear regression model\n",
        "model_tips = LinearRegression()\n",
        "model_tips.fit(X_train_tips, y_train_tips)\n",
        "\n",
        "# Print the model's coefficients, intercept, and R-squared score\n",
        "print(f\"Coefficient: {model_tips.coef_[0]}\")\n",
        "print(f\"Intercept: {model_tips.intercept_}\")\n",
        "print(f\"R-squared Score: {model_tips.score(X_test_tips, y_test_tips)}\")\n"
      ],
      "metadata": {
        "id": "nZrEWJ2jymO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6.\n",
        "# Fit the linear regression model\n",
        "model_tips = LinearRegression()\n",
        "model_tips.fit(X_tips, y_tips)\n",
        "\n",
        "# Print the slope and intercept\n",
        "print(f\"Slope: {model_tips.coef_[0]}\")\n",
        "print(f\"Intercept: {model_tips.intercept_}\")\n"
      ],
      "metadata": {
        "id": "ZpoHzMeQynV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X_synthetic = 2 * np.random.rand(100, 1)\n",
        "y_synthetic = 4 + 3 * X_synthetic + np.random.randn(100, 1)\n",
        "\n",
        "# Fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_synthetic, y_synthetic)\n",
        "\n",
        "# Predict new values\n",
        "X_new = np.array([[0], [2]])\n",
        "y_new = model.predict(X_new)\n",
        "\n",
        "# Plot the data points and regression line\n",
        "plt.scatter(X_synthetic, y_synthetic, color='blue')\n",
        "plt.plot(X_new, y_new, color='red', linewidth=2)\n",
        "plt.title('Linear Regression on Synthetic Data')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.show()\n",
        "\n",
        "# Print coefficients\n",
        "print(f\"Coefficient: {model.coef_[0][0]}\")\n",
        "print(f\"Intercept: {model.intercept_[0]}\")\n"
      ],
      "metadata": {
        "id": "5_5N_MOxypOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8.\n",
        "import pickle\n",
        "\n",
        "# Save the trained model using pickle\n",
        "with open('linear_regression_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "print(\"Model saved as 'linear_regression_model.pkl'\")\n"
      ],
      "metadata": {
        "id": "a9VcDpWmy3Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9.\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Generate synthetic data\n",
        "X_poly_synthetic = np.random.rand(100, 1) * 10\n",
        "y_poly_synthetic = 3 * X_poly_synthetic ** 2 + 2 * X_poly_synthetic + 1 + np.random.randn(100, 1) * 10\n",
        "\n",
        "# Transform the data for polynomial regression\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "X_poly = poly_features.fit_transform(X_poly_synthetic)\n",
        "\n",
        "# Fit the polynomial regression model\n",
        "poly_model = LinearRegression()\n",
        "poly_model.fit(X_poly, y_poly_synthetic)\n",
        "\n",
        "# Plot the polynomial regression curve\n",
        "plt.scatter(X_poly_synthetic, y_poly_synthetic, color='blue')\n",
        "plt.plot(X_poly_synthetic, poly_model.predict(X_poly), color='red')\n",
        "plt.title('Polynomial Regression (Degree 2)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "36GMlglVy4VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10.\n",
        "# Generate synthetic data\n",
        "X_lin = 2 * np.random.rand(100, 1)\n",
        "y_lin = 4 + 3 * X_lin + np.random.randn(100, 1)\n",
        "\n",
        "# Fit the linear regression model\n",
        "model_lin = LinearRegression()\n",
        "model_lin.fit(X_lin, y_lin)\n",
        "\n",
        "# Plot the data and regression line\n",
        "plt.scatter(X_lin, y_lin, color='blue')\n",
        "plt.plot(X_lin, model_lin.predict(X_lin), color='red')\n",
        "plt.title('Simple Linear Regression on Synthetic Data')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.show()\n",
        "\n",
        "# Print coefficients\n",
        "print(f\"Coefficient: {model_lin.coef_[0][0]}\")\n",
        "print(f\"Intercept: {model_lin.intercept_[0]}\")\n"
      ],
      "metadata": {
        "id": "38vUaVW1y5ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11.\n",
        "# Generate synthetic data\n",
        "X_poly3 = np.random.rand(100, 1) * 10\n",
        "y_poly3 = 2 * X_poly3 ** 3 + 3 * X_poly3 ** 2 + 4 * X_poly3 + 5 + np.random.randn(100, 1) * 50\n",
        "\n",
        "# Transform the data for polynomial regression (degree 3)\n",
        "poly_features3 = PolynomialFeatures(degree=3)\n",
        "X_poly3_transformed = poly_features3.fit_transform(X_poly3)\n",
        "\n",
        "# Fit the polynomial regression model\n",
        "poly_model3 = LinearRegression()\n",
        "poly_model3.fit(X_poly3_transformed, y_poly3)\n",
        "\n",
        "# Plot the polynomial regression curve\n",
        "plt.scatter(X_poly3, y_poly3, color='blue')\n",
        "plt.plot(X_poly3, poly_model3.predict(X_poly3_transformed), color='red')\n",
        "plt.title('Polynomial Regression (Degree 3)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "U6AxuBnlzPu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "12.\n",
        "# Generate synthetic data with two features\n",
        "X_two_features = np.random.rand(100, 2) * 10\n",
        "y_two_features = 3 * X_two_features[:, 0] + 2 * X_two_features[:, 1] + 4 + np.random.randn(100)\n",
        "\n",
        "# Fit the linear regression model\n",
        "model_two_features = LinearRegression()\n",
        "model_two_features.fit(X_two_features, y_two_features)\n",
        "\n",
        "# Print the coefficients, intercept, and R-squared score\n",
        "print(f\"Coefficients: {model_two_features.coef_}\")\n",
        "print(f\"Intercept: {model_two_features.intercept_}\")\n",
        "print(f\"R-squared Score: {model_two_features.score(X_two_features, y_two_features)}\")\n"
      ],
      "metadata": {
        "id": "uBbndOrXzP9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "13.\n",
        "# Generate synthetic data\n",
        "X_synth = 2 * np.random.rand(100, 1)\n",
        "y_synth = 5 + 3 * X_synth + np.random.randn(100, 1)\n",
        "\n",
        "# Fit the linear regression model\n",
        "model_synth = LinearRegression()\n",
        "model_synth.fit(X_synth, y_synth)\n",
        "\n",
        "# Predict on the synthetic dataset\n",
        "y_pred_synth = model_synth.predict(X_synth)\n",
        "\n",
        "# Calculate MSE, MAE, and RMSE\n",
        "mse_synth = mean_squared_error(y_synth, y_pred_synth)\n",
        "mae_synth = mean_absolute_error(y_synth, y_pred_synth)\n",
        "rmse_synth = np.sqrt(mse_synth)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Mean Squared Error (MSE): {mse_synth}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_synth}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_synth}\")\n"
      ],
      "metadata": {
        "id": "o9Nt3yH_zQIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "14.\n",
        "# Generate synthetic data with multiple features\n",
        "X_vif_synthetic = np.random.rand(100, 5) * 10\n",
        "X_vif_synthetic_df = pd.DataFrame(X_vif_synthetic, columns=['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5'])\n",
        "\n",
        "# Add constant for VIF calculation\n",
        "X_vif_synthetic_df = add_constant(X_vif_synthetic_df)\n",
        "\n",
        "# Calculate VIF for each feature\n",
        "vif_df = pd.DataFrame()\n",
        "vif_df['VIF Factor'] = [variance_inflation_factor(X_vif_synthetic_df.values, i) for i in range(X_vif_synthetic_df.shape[1])]\n",
        "vif_df['Feature'] = X_vif_synthetic_df.columns\n",
        "print(vif_df)\n"
      ],
      "metadata": {
        "id": "_PlzLMNUzQS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "15.\n",
        "# Generate synthetic data for a degree 4 polynomial relationship\n",
        "X_poly4 = np.random.rand(100, 1) * 10\n",
        "y_poly4 = 1 + 2 * X_poly4 + 3 * X_poly4 ** 2 + 4 * X_poly4 ** 3 + 5 * X_poly4 ** 4 + np.random.randn(100, 1) * 50\n",
        "\n",
        "# Transform the data for polynomial regression (degree 4)\n",
        "poly_features4 = PolynomialFeatures(degree=4)\n",
        "X_poly4_transformed = poly_features4.fit_transform(X_poly4)\n",
        "\n",
        "# Fit the polynomial regression model\n",
        "poly_model4 = LinearRegression()\n",
        "poly_model4.fit(X_poly4_transformed, y_poly4)\n",
        "\n",
        "# Plot the polynomial regression curve\n",
        "plt.scatter(X_poly4, y_poly4, color='blue')\n",
        "plt.plot(X_poly4, poly_model4.predict(X_poly4_transformed), color='red')\n",
        "plt.title('Polynomial Regression (Degree 4)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XhJrJR8CzQdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "16.\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data for multiple linear regression\n",
        "X_multi, y_multi = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the data\n",
        "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline_multi = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline\n",
        "pipeline_multi.fit(X_train_multi, y_train_multi)\n",
        "\n",
        "# Predict and evaluate\n",
        "r2_multi = pipeline_multi.score(X_test_multi, y_test_multi)\n",
        "print(f\"Pipeline R-squared Score: {r2_multi}\")\n"
      ],
      "metadata": {
        "id": "PzSCT8h-zQlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "17.\n",
        "# Generate synthetic data for a degree 3 polynomial relationship\n",
        "X_poly3_synth = np.random.rand(100, 1) * 10\n",
        "y_poly3_synth = 1 + 2 * X_poly3_synth + 3 * X_poly3_synth ** 2 + 4 * X_poly3_synth ** 3 + np.random.randn(100, 1) * 50\n",
        "\n",
        "# Transform the data for polynomial regression (degree 3)\n",
        "poly_features3_synth = PolynomialFeatures(degree=3)\n",
        "X_poly3_synth_transformed = poly_features3_synth.fit_transform(X_poly3_synth)\n",
        "\n",
        "# Fit the polynomial regression model\n",
        "poly_model3_synth = LinearRegression()\n",
        "poly_model3_synth.fit(X_poly3_synth_transformed, y_poly3_synth)\n",
        "\n",
        "# Plot the polynomial regression curve\n",
        "plt.scatter(X_poly3_synth, y_poly3_synth, color='blue')\n",
        "plt.plot(X_poly3_synth, poly_model3_synth.predict(X_poly3_synth_transformed), color='red')\n",
        "plt.title('Polynomial Regression (Degree 3)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OdnVYlHNzQti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "18.\n",
        "# Generate synthetic data for multiple linear regression\n",
        "X_synth_multi, y_synth_multi = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Fit the multiple linear regression model\n",
        "model_multi = LinearRegression()\n",
        "model_multi.fit(X_synth_multi, y_synth_multi)\n",
        "\n",
        "# Print the R-squared score and model coefficients\n",
        "print(f\"R-squared Score: {model_multi.score(X_synth_multi, y_synth_multi)}\")\n",
        "print(f\"Coefficients: {model_multi.coef_}\")\n"
      ],
      "metadata": {
        "id": "-hP9YB2GzQ1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "19.\n",
        "# Generate synthetic data for simple linear regression\n",
        "X_final = 2 * np.random.rand(100, 1)\n",
        "y_final = 5 + 2 * X_final + np.random.randn(100, 1)\n",
        "\n",
        "# Fit the linear regression model\n",
        "model_final = LinearRegression()\n",
        "model_final.fit(X_final, y_final)\n",
        "\n",
        "# Predict on the synthetic dataset\n",
        "y_pred_final = model_final.predict(X_final)\n",
        "\n",
        "# Plot the data points and regression line\n",
        "plt.scatter(X_final, y_final, color='blue', label='Data points')\n",
        "plt.plot(X_final, y_pred_final, color='red', label='Regression line')\n",
        "plt.title('Simple Linear Regression')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print the model's coefficient and intercept\n",
        "print(f\"Coefficient: {model_final.coef_[0][0]}\")\n",
        "print(f\"Intercept: {model_final.intercept_[0]}\")\n"
      ],
      "metadata": {
        "id": "odXbX6GczQ8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "20.\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate synthetic data with 3 features\n",
        "X_3_features, y_3_features = make_regression(n_samples=100, n_features=3, noise=0.1, random_state=42)\n",
        "\n",
        "# Fit the multiple linear regression model\n",
        "model_3_features = LinearRegression()\n",
        "model_3_features.fit(X_3_features, y_3_features)\n",
        "\n",
        "# Print the R-squared score and coefficients\n",
        "print(f\"R-squared Score: {model_3_features.score(X_3_features, y_3_features)}\")\n",
        "print(f\"Coefficients: {model_3_features.coef_}\")\n"
      ],
      "metadata": {
        "id": "UitTHf2qzRD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "21.\n",
        "# Save the model to a file\n",
        "with open('linear_regression_model.pkl', 'wb') as file:\n",
        "    pickle.dump(model_3_features, file)\n",
        "\n",
        "# Load the model from the file\n",
        "with open('linear_regression_model.pkl', 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "\n",
        "# Predict using the loaded model\n",
        "predictions = loaded_model.predict(X_3_features[:5])\n",
        "print(\"Predictions on the first 5 samples:\", predictions)\n"
      ],
      "metadata": {
        "id": "YqlCrfKy0nu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "22.\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load the 'tips' dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# One-hot encode the categorical features\n",
        "encoder = OneHotEncoder(drop='first', sparse=False)\n",
        "encoded_features = encoder.fit_transform(tips[['sex', 'smoker', 'day', 'time']])\n",
        "\n",
        "# Create the feature matrix and target vector\n",
        "X_tips = pd.concat([pd.DataFrame(encoded_features), tips[['total_bill', 'size']]], axis=1)\n",
        "y_tips = tips['tip']\n",
        "\n",
        "# Fit the linear regression model\n",
        "model_tips = LinearRegression()\n",
        "model_tips.fit(X_tips, y_tips)\n",
        "\n",
        "# Print the R-squared score and coefficients\n",
        "print(f\"R-squared Score: {model_tips.score(X_tips, y_tips)}\")\n",
        "print(f\"Coefficients: {model_tips.coef_}\")\n"
      ],
      "metadata": {
        "id": "zQ-CrcZP0pOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "23.\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Generate synthetic data\n",
        "X_ridge, y_ridge = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Fit the Linear Regression model\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_ridge, y_ridge)\n",
        "\n",
        "# Fit the Ridge Regression model\n",
        "ridge_model = Ridge(alpha=1.0)\n",
        "ridge_model.fit(X_ridge, y_ridge)\n",
        "\n",
        "# Print the R-squared scores and coefficients\n",
        "print(f\"Linear Regression R-squared: {linear_model.score(X_ridge, y_ridge)}\")\n",
        "print(f\"Linear Regression Coefficients: {linear_model.coef_}\")\n",
        "\n",
        "print(f\"Ridge Regression R-squared: {ridge_model.score(X_ridge, y_ridge)}\")\n",
        "print(f\"Ridge Regression Coefficients: {ridge_model.coef_}\")\n"
      ],
      "metadata": {
        "id": "WLH8aF6y1AMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "24.\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Create a synthetic dataset\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Initialize the model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Perform cross-validation and calculate the R-squared score for each fold\n",
        "cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "\n",
        "# Print the cross-validation R-squared scores and the mean score\n",
        "print(f\"Cross-validation R-squared scores: {cv_scores}\")\n",
        "print(f\"Mean R-squared score: {np.mean(cv_scores)}\")\n"
      ],
      "metadata": {
        "id": "i8RElWsH1AYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "25.\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Create a synthetic dataset\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Evaluate polynomial regression models for different degrees\n",
        "for degree in range(1, 6):\n",
        "    # Transform features into polynomial features\n",
        "    poly = PolynomialFeatures(degree)\n",
        "    X_poly_train = poly.fit_transform(X_train)\n",
        "    X_poly_test = poly.transform(X_test)\n",
        "\n",
        "    # Initialize the model\n",
        "    model = LinearRegression()\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_poly_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_poly_test)\n",
        "\n",
        "    # Calculate the R-squared score\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"Degree {degree} Polynomial Regression R-squared: {r2}\")\n"
      ],
      "metadata": {
        "id": "8ZGdFk_r1AkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "26.\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Create a synthetic dataset with multiple features\n",
        "X, y = make_regression(n_samples=100, n_features=3, noise=0.1, random_state=42)\n",
        "\n",
        "# Add interaction terms to the dataset (degree=2 for interaction terms)\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=False)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Initialize the linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model with the transformed features\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Print the coefficients (including interaction terms)\n",
        "print(\"Coefficients with interaction terms:\")\n",
        "print(model.coef_)\n",
        "\n",
        "# Optionally, print the names of the features (including interaction terms)\n",
        "print(\"Feature names:\")\n",
        "print(poly.get_feature_names_out())\n"
      ],
      "metadata": {
        "id": "wMZdaHDI1AwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n8K3iW84vXBV"
      }
    }
  ]
}